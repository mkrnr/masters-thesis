\chapter{Conditional Random Fields}\label{cha:crfs}

As we discussed in the previous chapter, \glspl{crf} are widely used in the related work for learning probabilistic models on given data.
In this chapter we give an introduction to this framework.
First we provide an overview of relevant concepts in probability theory, graph theory, and graphical models.
In addition to relevant definitions we will use a simplified example (see \todo{ref to Appendix}) that is based on the extraction of author information from reference strings in research papers.
Following this we will introduce the concept of \glspl{crf} and discuss the inference and learning of \gls{crf} models for the task of entity recognition.\\


\section{Foundations}\label{sec:foundations}
\subsection{Probability Theory}\label{subsec:probability-theory}
Several concepts from probability theory are crucial for an understanding of \glspl{crf}.
The notion of \glspl{probability distribution} is one such concept.
A \gls{probability distribution} $P$ is defined over $(\glssymbol{outcome space},\glssymbol{measurable set})$ where \glssymbol{outcome space} is the \gls{outcome space} and \glssymbol{measurable set} is a \gls{measurable set} of \glspl{event} \citep{koller2009probabilistic}.
$P$ describes a mapping from events in \glssymbol{measurable set} to real values according to the following rules \citep{koller2009probabilistic}:
\begin{itemize}
  \item $P(\alpha)\geq 0 $ for all $ \alpha \in S$.
  \item $P(\glssymbol{outcome space})=1$.
  \item If $\alpha,\beta\in \glssymbol{outcome space}$ and $\alpha\cap\beta = \emptyset$, then $P(\alpha\cup\beta)=P(\alpha)+P(\beta)$.
\end{itemize}

In our example of author information extraction we define \glssymbol{outcome space} as the set of possible word sequences that can appear in a given text line of a research paper.
For the purpose of illustration we consider four events:

Event $firstPeriod$ contains all word sequences in which the first word of the sequence ends with a period and event $firstNotPeriod$ contains $\Omega\ -\ firstPeriod$.
Event $secondCapitalized$ contains all word sequences in which the second word of the sequence is capitalized and event $secondNotCapitalized$ is defined accordingly.

\glssymbol{measurable set} contains the four defined events and per definition also $\emptyset$ and $\Omega$.
$P$ now assigns all events in \glssymbol{measurable set} a probability.
Given the values in \todo{add ref}, $P(firstPeriod)$ is $XXX$ and $P(secondNotCapitalized)$ is $XXX$.\\

Based on the idea of \glspl{probability distribution}, a \gls{random variable} is a \gls{function} that associates with each outcome in \glssymbol{outcome space} a value~\cite{koller2009probabilistic}.
A set of \glspl{random variable} is denoted with bold capital letters and assignments of values to the \glspl{random variable} in such sets are denoted with bold lowercase letters~\cite{koller2009probabilistic}.
In addition, $Val(X)$ is the set of values that $X$ can take.
Given a \gls{random variable} $X$, $P(X)$ is the distribution over \glspl{event} that are described using $X$~\cite{koller2009probabilistic}.
It is referred to as the \gls{marginal distribution} over $X$.
Given a set of \glspl{random variable} $\bm{X}$, $P(\bm{X})$ is called \gls{joint distribution} and assigns each \gls{full assignment} to $\bm{X}$ a probability~\cite{koller2009probabilistic}.
A \gls{full assignment} $\xi$ assigns to every \gls{random variable} in $\bm{X}$ a value and thereby $\xi\in Val(\bm{X})$.
Any \gls{event} that is described using $\bm{X}$ must be a union of events that correspond to \glspl{full assignment} to $\bm{X}$.
This gives us a \gls{canonical outcome space} where each outcome is a joint assignment to the \glspl{random variable} in $\bm{X}$.

Following our example we define a \gls{random variable} $Y$ as the \gls{function} that associates with the first word in a sequence whether or not it end with a period.
Thereby, the \gls{event} $firstPeriod$ can now also be denoted with $Y = period$ and $firstNotPeriod$ with $Y = notPeriod$ resulting in $Val(Y)=\{period, notPeriod\}$.
Consequently, $P(firstPeriod)=P(Y=period)$ and $P(firstNotPeriod)=P(Y=period)$.
$X$ is defined as the function that associates with the second word in a sequence whether or not it is capitalized with $Val(X)=\{capitalized, notCapitalized\}$.
Given $X$ and $Y$, we can build the \gls{joint distribution} $P(X,Y)$.
Using the values from \todo{ref}, $P(X=capitalized,Y=period)=XXX$ and $P(X=capitalized,Y=notPeriod)=XXX$.\\

The \gls{conditional probability} of an event $\alpha$ given $\beta$ with $P(\beta)>0$ is defined as~\cite{koller2009probabilistic}:

\begin{equation}
P(\alpha|\beta) = \frac{P(\alpha\cap\beta)}{P(\beta)}
\label{equ:conditional-probability}
\end{equation}

This definition can be extended to \glspl{random variable}.
Given that all assignments in $Y$ are non-zero, the \gls{conditional probability distribution} $P(X|Y)$ assigns each value $Y$ a probability over values of $X$ using the \gls{conditional probability}.
This can also be extended to sets of \glspl{random variable}.

With our example values from \todo{ref}, $P(X=capitalized\ |\ Y=period)=xxx$ and $P(X=capitalized\ |\ Y=notPeriod)=xxx$.\\

Another relevant concept is the notion of \gls{independence}.
An \gls{event} $\alpha$ is \glslink{independence}{independent} of \gls{event} $\beta$ in $P$ if $P(\alpha\mid\beta)=P(\alpha)$ or if $P(\beta)=0$~\cite{koller2009probabilistic}.
The \gls{independence} of $\alpha$ and $\beta$ is denoted with $P\models(\alpha\perp\beta)$.
Two \glspl{event} can also be \glslink{independence}{independent} given a third \gls{event}, called \gls{conditional independence}:
$\alpha$ is \glslink{conditional independence}{conditionally independent} of \gls{event} $\beta$ given \gls{event} $\gamma$ in $P$ if $P(\alpha\mid\beta\cap\gamma)=P(\alpha\mid\gamma)$ or if $P(\beta\cap\gamma)=0$~\cite{koller2009probabilistic}.
\Gls{conditional independence} is denoted with $P\models(\alpha\perp\beta\mid\gamma)$.

\itodo{add example for independence and conditional independence}

\itodo{probability query}

\subsection{Graph Theory}\label{subsec:graph-theory}

%TODO definition graphs, nodes, edges
This subsection will give brief overview of concepts from graph theory that are needed for describing \glspl{crf} and graphical models in general.\\

% TODO add explanation for converting directed graph to undirected by ignoring the directions of the edges?
A \gls{graph} $\mathcal{K}$ consists of a set of \glspl{node} (also called vertices) $\mathcal{V}$ and a set of \glspl{edge} $\mathcal{E}$.
In a directed \gls{graph}, a pair of \glspl{node} $(v_i,v_j)$ can be connected by directed \glspl{edge} $v_i\to v_j$.
We write $v_i\rightleftharpoons v_j$ to denote that there is some directed edge between $v_i$ and $v_j$.
The \glspl{node} in an undirected graph are connected by undirected \glspl{node} $v_i\undedge v_j$.
In the following, directed \glspl{graph} are denoted with $\mathcal{G}$ and undirected \glspl{graph} with $\mathcal{H}$.
% TODO about child and parent?

Given a \gls{graph} $\mathcal{K} = (\mathcal{V},\mathcal{E})$ and $\mathcal{S}\in\mathcal{V}$, an induced \gls{subgraph} $\mathcal{K}[\mathcal{S}]$ is the \gls{graph} consisting of $(\mathcal{S},\mathcal{E'})$ where $\mathcal{E'}$ is the set of all \glspl{edge} between \glspl{node} in $\mathcal{S}$.
In a complete \gls{subgraph}, every two \glspl{node} in $\mathcal{S}$ are connected by an \gls{edge}.
% TODO add definition of clique?

\subsection{Probabilistic Graphical Models}\label{subsec:graphical-models}
When encoding practical problems with \glspl{probability distribution}, an observation is that ``variables tend to interact only with a very few others''~\citep{koller2009probabilistic}.
This makes it possible to represent such distributions as graphs in a tractable and transparent way, allowing domain experts to evaluate their properties~\citep{koller2009probabilistic}.
\Glspl{probabilistic graphical model} are such representations.\\
%TODO edge part more precise

In a \gls{probabilistic graphical model}, the set of \glspl{random variable} $\mathcal{X}=\{X_1,\dots,X_n\}$ of a distribution are modeled as \glspl{node} and \glspl{edge} denote a ``direct probabilistic interaction''~\citep{koller2009probabilistic} between the two neighboring nodes.
There are two fundamental groups of graphical models, based on the type of edge that are used:

\Glspl{bayesian network} are encoded with directed \glspl{edge} to build a directed acyclic graph~\citep{koller2009probabilistic}.
An \gls{edge} $X_i\to X_j$ thereby models a direct influence of $X_i$ on $X_j$.
Referring to the example from \Cref{subsec:probability-theory}, $X\to Y$ denotes the influence of the first word in a sequence ending with a period on the second word being capitalized.
Using the chain rule from \todo{add chain rule} it is possible to separate the \gls{joint distribution} $P(\mathcal{X})$ into a set of \glspl{prior distribution} and \glspl{cpd} by considering the direct influences between the \glspl{node}.
See \todo{add} for such a separation of the \gls{joint distribution} $P(X,Y)$ of our example.

\Glspl{markov network} on the other hand use undirected \glspl{edge} to model a symmetrical influence between two \glspl{random variable}.
Because of this it is not possible to separate the \gls{joint distribution} $P(\mathcal{X})$ into \glspl{prior distribution} and \glspl{cpd}.
Instead, $P(\mathcal{X})$ is represented as the product of \glspl{factor}.
Given a set of \glspl{random variable} $\bm{D}$, a \gls{factor} $\phi$ is a function from $Val(\bm{D})$ to $\realnumbers$ and it is called nonnegative if all its entries are nonnegative~\citep{koller2009probabilistic}. $\bm{D}$ is called the \glslink{factor scope}{scope} of $\phi$, denoted by $Scope[\phi]$.
For an example of a factor see \todo{add}.

In the following chapters we will see that besides the encoding of practical distributions, \glspl{probabilistic graphical model} also support learning in a data-driven manner and allow effective inference.

\section{Definition of CRFs}\label{sec:definition-crfs}
% TODO observed & target variables

\section{Entity Recognition using CRFs}\label{sec:er-using-crfs}

