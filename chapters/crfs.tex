\chapter{Conditional Random Fields (\glsentryshortpl{crf})}\label{cha:crfs}

As we discussed in the previous chapter, \glspl{crf} are widely used in the related work for learning probabilistic models on given data.
In this chapter we give an introduction to this framework.
First we provide an overview of relevant concepts in probability theory, graph theory, and graphical models.
In addition to relevant definitions we will use a simplified example (see \todo{ref to Appendix}) that is based on the extraction of author information from reference strings in research papers.
Following this we will introduce the concept of \glspl{crf} and discuss the inference and learning of \gls{crf} models for the task of entity recognition.

\section{Foundations}\label{sec:foundations}
\subsection{Probability Theory}\label{subsec:probability-theory}
Several concepts from probability theory are crucial for an understanding of \glspl{crf} and they all build on the notion of \glspl{probability distribution}.

A \gls{probability distribution} $P$ is defined over $(\glssymbol{outcome space},\glssymbol{measurable set})$ where \glssymbol{outcome space} is the \gls{outcome space} and \glssymbol{measurable set} is a \gls{measurable set} of \glspl{event} \citep{koller2009probabilistic}.
$P$ describes a mapping from events in \glssymbol{measurable set} to real values according to the following rules \citep{koller2009probabilistic}:
\begin{itemize}
  \item $P(\alpha)\geq 0 $ for all $ \alpha \in S$.
  \item $P(\glssymbol{outcome space})=1$.
  \item If $\alpha,\beta\in \glssymbol{outcome space}$ and $\alpha\cap\beta = \emptyset$, then $P(\alpha\cup\beta)=P(\alpha)+P(\beta)$.
\end{itemize}
In our example of author information extraction we define \glssymbol{outcome space} as the set of possible word sequences that can appear in a given text line of a research paper.
For the purpose of illustration we consider four events:

%TODO replace "`firstPeriod"'
Event $firstComma$ contains all word sequences in which the first word of the sequence ends with a comma and event $firstNotComma$ is defined accordingly.
Event $secondFirstName$ contains all word sequences in which the second word of the sequence is a first name and event $secondNotFirstName$ again is defined accordingly.

\glssymbol{measurable set} contains the four defined events and per definition also $\emptyset$ and $\Omega$.
$P$ now assigns all events in \glssymbol{measurable set} a probability.
Given the values in \todo{add ref}, $P(firstComma)$ is $XXX$ and $P(secondFirstName)$ is $XXX$.

\bigskip

Based on the idea of \glspl{probability distribution}, a \gls{random variable} $X$ is a \gls{function} that associates with each outcome in \glssymbol{outcome space} a value~\cite{koller2009probabilistic}.
In addition, $Val(X)$ is the set of values that $X$ can take.
Such a value $x$ is also referred to as an assignment to $X$.
We can define a vector of \glspl{random variable} $\mathbf{X}=\left\langle X_1,\dots,X_n\right\rangle$ and accordingly a vector of assignments $\mathbf{x}=\left\langle x_1,\dots,x_n\right\rangle$ to $\mathbf{X}$.
Given a \gls{random variable} $X$, $P(X)$ is the \gls{probability distribution} over $Val(X)$~\cite{koller2009probabilistic}.
It is referred to as the \gls{marginal distribution} over $X$.
Given a set of \glspl{random variable} $\mathbf{X}$, $P(\mathbf{X})$ is called \gls{joint distribution} and assigns each \gls{full assignment} $\mathbf{x}$ to $\mathbf{X}$ a probability~\cite{koller2009probabilistic}.
$Val(\bm{X})$ is the set of possible \glspl{full assignment} to $\mathbf{X}$.
Any \gls{event} that is described using $\mathbf{X}$ must be a union of \glspl{full assignment} to $\mathbf{X}$\todo{example}.
This gives us a \gls{canonical outcome space} where each outcome is a joint assignment to the \glspl{random variable} in $\mathbf{X}$.

Following our example we define a \gls{random variable} $X_1$ as the \gls{function} that associates with the first word in a sequence the probability of it ending with a comma.
Thereby, the \gls{event} $firstComma$ can now also be denoted with $X_1 = comma$ and $firstNotComma$ with $X_1 = notComma$ resulting in $Val(X_1)=\{comma, notComma\}$.
Consequently, $P(firstComma)=P(X_1{=}comma)$ and $P(firstNotComma)=P(X_1{=}notComma)$.
$Y_2$ is defined as the function that associates with the second word in a sequence whether or not it is a first name with $Val(Y_2)=\{firstName, notFirstName\}$.
Given $X_1$ and $Y_2$, we can build the \gls{joint distribution} $P(X_1,Y_2)$.
Using the values from \todo{ref}, $P(X_1{=}comma,Y_2{=}firstName)=XXX$ and $P(X_1{=}comma,Y_2{=}notFirstName)=XXX$.

\bigskip

The \gls{conditional probability} of an event $\alpha$ given $\beta$ with $P(\beta)>0$ is defined as~\cite{koller2009probabilistic}:

\begin{equation}
\label{equ:conditional-probability}
P(\alpha\mid\beta) = \frac{P(\alpha\cap\beta)}{P(\beta)}
\end{equation}
This definition can be extended to \glspl{random variable}.
Given that all assignments in $Y$ are non-zero, the \gls{conditional probability distribution} $P(X\mid Y)$ assigns each value $Y$ a probability over values of $X$ using the \gls{conditional probability}.
This can also be extended to sets of \glspl{random variable}.

With our example values from \todo{ref}, $P(Y_2{=}firstName\mid X_1{=}comma)=XXX$ and $P(Y_2{=}firstName\mid X_1{=}notComma)=xxx$.

\bigskip

A value that we will later on use as a metric to compare different \glspl{probability distribution} with each other is called \gls{expectation}.
Given a discrete \gls{random variable} $X$ we define the expectation $E[X]$ of $X$ under the distribution $P$ as~\cite{koller2009probabilistic}
\begin{equation}
  \label{equ:expectation-x}
  E[X]=\sum_{x\in Val(X)} x\cdot P(x)
\end{equation}
where $x$ is an assignment to $X$.

\bigskip

Given a \glspl{probability distribution} over a set of \glspl{random variable} $\mathcal{X}$, a goal can be to answer specific questions about this distribution.
These are formulated as queries which are then run against the \glslink{probability distribution}{distributions}.
One type of queries are called \glspl{probability query}.
A \gls{probability query} consists of $\mathbf{E}\cup\mathbf{Y}=\mathcal{X}$ with $\mathbf{E}\cap\mathbf{Y}=\emptyset$~\citep{koller2009probabilistic}.
The set of \glspl{random variable} $\mathbf{E}$ is called the \gls{evidence} and contains the set of observed variables.
The second set $\mathbf{Y}$ contains the query variables.
Based on this, the query is formulated as $P(\mathbf{Y}|\mathbf{E}=\mathbf{e})$~\citep{koller2009probabilistic}\todo{notation?}.
We thereby want to compute the posterior \gls{probability distribution} over the assignments $\mathbf{y}$ to $\mathbf{Y}$, given the conditioning $\mathbf{E}=\mathbf{e}$~\citep{koller2009probabilistic}.
\itodo{better explanation}

\subsection{Graph Theory}\label{subsec:graph-theory}

This subsection will give brief overview of concepts from graph theory that are needed for describing \glspl{crf} and graphical models in general.

\bigskip

% TODO add explanation for converting directed graph to undirected by ignoring the directions of the edges?
A \gls{graph} $\mathcal{K}$ consists of a set of \glspl{node} (also called vertices) $\mathcal{V}$ and a set of \glspl{edge} $\mathcal{E}$.
In a directed \gls{graph}, a pair of \glspl{node} $(v_i,v_j)$ can be connected by directed \glspl{edge} $v_i\to v_j$.
We write $v_i\rightleftharpoons v_j$ to denote that there is some directed edge between $v_i$ and $v_j$.
The \glspl{node} in an undirected graph are connected by undirected \glspl{node} $v_i\undedge v_j$.
In the following, directed \glspl{graph} are denoted with $\mathcal{G}$ and undirected \glspl{graph} with $\mathcal{H}$.
% TODO about child and parent?

Given a \gls{graph} $\mathcal{K} = (\mathcal{V},\mathcal{E})$ and $\mathcal{S}\in\mathcal{V}$, an induced \gls{subgraph} $\mathcal{K}[\mathcal{S}]$ is the \gls{graph} consisting of $(\mathcal{S},\mathcal{E'})$ where $\mathcal{E'}$ is the set of all \glspl{edge} between \glspl{node} in $\mathcal{S}$.
In a complete \gls{subgraph}, every two \glspl{node} in $\mathcal{S}$ are connected by an \gls{edge}.
% TODO add definition of clique?

\subsection{Probabilistic Graphical Models}\label{subsec:graphical-models}
When encoding practical problems with \glspl{probability distribution}, a key insight is that ``variables tend to interact only with a very few others''~\citep{koller2009probabilistic}.
This makes it possible to represent such distributions as graphs in a tractable and transparent way, allowing domain experts to evaluate their properties~\citep{koller2009probabilistic}.
\Glspl{probabilistic graphical model} are such representations.

\bigskip

In a \gls{probabilistic graphical model}, the \glspl{random variable} $\mathcal{X}$ of a distribution are modeled as \glspl{node} where each \gls{node} represents one \gls{random variable} in $\mathcal{X}$. An \glspl{edge} then denotes a ``direct probabilistic interaction''~\citep{koller2009probabilistic} between its two two incident \glspl{node}.

% TODO edge part more precise
There are two fundamental groups of graphical models, based on the type of edge that are used: \glspl{bayesian network} and \glspl{markov network}.

\bigskip

\Glspl{bayesian network}, usually denoted with $\mathcal{G}$, are encoded with directed \glspl{edge} to build a directed acyclic graph~\citep{koller2009probabilistic}.
An \gls{edge} $X_i\to X_j$ thereby models a direct influence of $X_i$ on $X_j$.
Referring to the example from \Cref{subsec:probability-theory}, $X\to Y$ denotes the influence of the first word in a sequence ending with a period on the second word being capitalized.
Using the chain rule from \todo{add chain rule} it is possible to separate the \gls{joint distribution} $P(\mathcal{X})$ into a set of \glspl{prior distribution} and \glspl{cpd} by considering the direct influences between the \glspl{node}.
See \todo{add} for such a separation of the \gls{joint distribution} $P(X,Y)$ of our example.

\bigskip

\Glspl{markov network}, usually denoted with $\mathcal{H}$, use undirected \glspl{edge} to model a symmetrical influence between two \glspl{random variable}.
Because of this it is not possible to separate the \gls{joint distribution} $P(\mathcal{X})$ into \glspl{prior distribution} and \glspl{cpd}.
Instead, $P(\mathcal{X})$ is represented as the product of \glspl{factor}.
A \gls{factor} $\Psi(\mathbf{d})$ is a function from $\mathbf{d}$ to $\realnumbers$ where $\mathbf{d}\in Val(\mathbf{D})$~\citep{koller2009probabilistic}.
It is called nonnegative if all its entries are nonnegative and in the following we will consider all factors to be nonnegative.
$\mathbf{D}$ is called the \glslink{factor scope}{scope} of $\Psi$, denoted by $Scope[\Psi]$.
For an example of a factor see \todo{add}.

An important operation on factors is the \gls{factor product}.
Given three disjoint sets of assignments $\left\{\mathbf{x}, \mathbf{y}, \mathbf{z}\right\}$, a \gls{factor product} $\Psi_1\times \Psi_2$ of the two factors $\Psi_1(\mathbf{x},\mathbf{y})$ and $\Psi_2(\mathbf{y},\mathbf{z})$ is a factor $\Psi(\mathbf{x},\mathbf{y},\mathbf{z})$~\citep{koller2009probabilistic}.
The intuition is that the two factors are multiplied by aligning their common set of assignments $\mathbf{y}\in Val(\mathbf{Y})$\todo{example}.

Based on the definition of a \gls{factor product}, we can define an undirected parameterization of a \gls{probability distribution}, called \gls{gibbs distribution}.
A \gls{probability distribution} $P(x_1,\dots,x_N)$ is a \gls{gibbs distribution} parameterized by a set of \glspl{factor} $\{\Psi_1(\mathbf{d}_1),\dots,\Psi_K(\mathbf{d}_K)\}$ if it is defined as~\citep{koller2009probabilistic}
\begin{equation}
  \label{equ:gibbs-distribution}
  \begin{split}
  P\left(x_1,\dots,x_N\right) & =\frac{1}{Z}\tilde{P}\left(x_1,\dots,x_N\right) \\
  \tilde{P}\left(x_1,\dots,x_N\right) & =\prod_{k=1}^{K}\Psi_k\left(\mathbf{d}_k\right) \\
  Z & =\sum_{x_1,\ldots,x_N}\tilde{P}\left(x_1,\dots,x_N\right)
  \end{split}
\end{equation}
where is a $\tilde{P}(x_1,\dots,x_N)$ is an unnormalized measure and $Z$ is a normalizing constant, sometimes called the \gls{partitioning function}, which guarantees that the \gls{probability distribution} sums to 1.
Having a \gls{gibbs distribution} $P$ where each $\mathbf{d}_k$, $1\leq k \leq K$ is a complete subgraph of a \gls{markov network} $\mathcal{H}$, we say that $P$ factorizes over $\mathcal{H}$~\citep{koller2009probabilistic}\todo{example?}.

One could think that \glspl{factor} are assigned to the edges of a \gls{markov network} in order to parameterize it.
Yet, such an assignment is only able to capture the pairwise interactions between the two incident nodes~\citep{koller2009probabilistic}.
In order to model more complex interactions involving multiple nodes, the \glspl{factor scope} needs to allow an arbitrary subset of nodes in $H$.

Since a factor can be assigned to an arbitrary number of nodes, visualizing \glspl{markov network} by only displaying the random variables as nodes and the independence relations as edges is not sufficient.
Instead, a \gls{factor graph} can be used.
A \gls{factor graph} $\mathcal{F}$ contains two types of nodes: Oval nodes denote variable nodes and squared nodes denote factor nodes~\citep{koller2009probabilistic}.
Each factor node is associated with exactly one factor $\Psi$ and each variable node is associated with exactly one random variable.
The graph only contains undirected edges between factor nodes and variable nodes and the scope of $\Psi$ is the set of variables that are adjacent to its corresponding factor node~\citep{koller2009probabilistic}\todo{example}.

As we have discussed before, \glspl{factor} are encoded with tables that contain a nonnegative value for each possible assignments in its \glslink{factor scope}{scope}.
Having a set of \glspl{factor} that parameterize a graph. A probability distribution
Another way to parameterize \glspl{factor} is by converting them into log-space~\citep{koller2009probabilistic}.
We can rewrite a factor $\Psi(\mathbf{d})$ as
\begin{equation*}
  \label{equ:energy-function}
  \Psi(\mathbf{d}) = \exp(-\epsilon(\mathbf{d}))
\end{equation*}
where $\epsilon(\mathbf{d})=-\ln\Psi(\mathbf{d})$ is called \gls{energy function}~\citep{koller2009probabilistic}.

Since we are in the log-space, we can calculate a \gls{probability distribution} over a set of \glspl{random variable} the following way~\citep{koller2009probabilistic}:
\begin{equation}
  \label{equ:p-energy-function}
  P\left(x_1,\dots,x_N\right) \propto \exp\left\{-\sum_{k=1}^K\epsilon_k\left(\mathbf{d}_k\right)\right\}
\end{equation}
Since we only consider nonnegative \glspl{factor} and the results of the \gls{energy function} can be negative, we need a new definition of a \gls{factor} without its negativity property.
Thereby, given a set of \glspl{random variable} $\mathbf{d}$, we define a \gls{feature function} $f(\mathbf{d})$ as a function from $\mathbf{d}$ to $\realnumbers$~\citep{koller2009probabilistic}.
In \todo{example} we apply the \gls{energy function} to the values of \todo{example}.
Values that were set to one in \todo{ref} become zero. \dots\todo{differences}
Consequently following this separation we can define a \gls{probability distribution} $P$ over $\mathcal{H}$ with~\citep{koller2009probabilistic}:
\begin{equation}
  \label{equ:log-linear-model}
  P\left(x_1,\dots,x_N\right) = \frac{1}{Z}\exp\left\{-\sum_{k=1}^K \theta_k f_k\left(\mathbf{d}_k\right)\right\}
\end{equation}
where $\theta_1,\dots,\theta_K$ are weights and $\{f_1(\mathbf{d}_1),\dots,f_K(\mathbf{d}_K)\}$ are \glspl{feature function} with each $\mathbf{d}_k$ being a complete subgraph in $\mathcal{H}$.
This \gls{probability distribution} is called a \gls{log-linear model}.

\bigskip

In addition to \glspl{bayesian network} and \glspl{markov network} we can further distinguish between \glspl{generative model} and \glspl{discriminative model}.
Assume a set of input variables, also called \glspl{observed variable}, $\mathbf{X}$ and a set of output variables, also called \glspl{target variable} $\mathbf{Y}$ with $\mathbf{X}\cap\mathbf{Y}=\emptyset$.
A \gls{generative model} then encodes the \gls{joint distribution} $P(\mathbf{y},\mathbf{x})$ whereas a \gls{discriminative model} encodes the \gls{conditional probability distribution} $P(\mathbf{y}\mid\mathbf{x})$~\citep{koller2009probabilistic}.
More precisely, for a \gls{generative model} we have $P(\mathbf{y},\mathbf{x})=P(\mathbf{y})P(\mathbf{x}\mid\mathbf{y})$.
We thereby consider how the output of the model is generated as a function of the input~\citep{sutton2010introduction}.

This leads to the main difference between the two models, namely that for a \gls{discriminative model} we do not need to model $P(\mathbf{x})$.
\citet{sutton2010introduction} argue that indeed the modeling of $P(\mathbf{x})$ in \glspl{generative model} leads to a number of difficulties and limitations.
According to them, $P(\mathbf{x})$ often contains a number of highly dependent features which restrict the modeling.
For example, in \gls{nlp} tasks we often model word-identities as features.
Having a limited training set, we often have words that were unseen during training.
In order to still give a reasonable classification for unseen words it would be beneficial to also include other features in addition to just the word-identities~\citep{sutton2010introduction}.
Such features could be the capitalization, length, and prefixes of a word or even its neighboring words~\citep{sutton2010introduction}.
Yet, such features are highly dependent to each other and such dependencies would need to be represented in a \gls{generative model} which is often intractable in practice~\citep{sutton2010introduction}.
\Glspl{discriminative model} on the other hand can leverage such features despite their high dependencies since $P(\mathbf{x})$ is not modeled~\citep{koller2009probabilistic}.
%TODO argue about modeling p(Y|X) directly since it represents our probability query (sutton?)

Since in \glspl{generative model} $P(\mathbf{y})$ topologically precedes\todo{what?} $P(\mathbf{x}\mid\mathbf{y})$, \citet{sutton2010introduction} argue that they are more naturally modeled by a \gls{bayesian network}.
Since there is no such order in \glspl{discriminative model}, it is argued that they are more naturally modeled by a \gls{markov network}~\citep{sutton2010introduction}.

\bigskip

After introducing some of the fundamental concepts we can now discuss \glspl{crf}. In the following we will address how to encode, inference, and learn them.

\section{Encoding of \glsentryshortpl{crf}}\label{sec:definition-crfs}
A popular framework for building \glspl{probabilistic graphical model} are \acrfullpl{crf}.
Proposed by \citet{lafferty2001conditional}, the initial goal was the segmentation and labeling sequence data.
A main motivation for \glspl{crf} is to overcome a label bias problem that other discriminative \glspl{markov network}, such as \glspl{memm}, tend to have~\citep{lafferty2001conditional}.\todo{rewrite}
\citet{lafferty2001conditional} argue that this is due to the structure in which models such as \glspl{memm} model \glspl{conditional probability} using per-state models.
This can lead to a bias towards stats which have fewer outgoing transitions~\citep{lafferty2001conditional}.
In order to overcome this bias, \glspl{crf} do not have per-state models but instead contain a single model to represent ``the \glslink{joint distribution}{joint probability} of the entire sequence of labels given the observation sequence''~\citep{lafferty2001conditional}.

\bigskip

\Glspl{crf} encode the \gls{conditional probability distribution} $P(\mathbf{y}\mid\mathbf{x})$ where $\mathbf{y}$ is a set of assignments to \glspl{target variable} and $\mathbf{x}$ is a set of \glspl{observed variable} with $\mathbf{y}\cap\mathbf{x}=\emptyset$.
A \gls{crf} is constructed using a \gls{markov network} $\mathcal{H}$ where the nodes correspond to $\mathbf{Y}\cup\mathbf{X}$ and the undirected edges model a symmetrical influence between the nodes~\citep{koller2009probabilistic}.
Given a set of \glspl{factor} $\{\Psi_1(\mathbf{d}_1),\dots\Psi_K(\mathbf{d}_K)\}$ that factorize over $\mathcal{H}$, a \gls{crf} defines $P(\mathbf{y}\mid\mathbf{x})$ as~\citep{koller2009probabilistic}
\begin{equation}
  \label{equ:crf-factor}
  \begin{split}
    P(\mathbf{y}\mid\mathbf{x}) & = \frac{1}{Z(\mathbf{x})}\tilde{P}(\mathbf{y},\mathbf{x}) \\
    \tilde{P}(\mathbf{y},\mathbf{x}) &= \prod_{k=1}^{K}\Psi_k\left(\mathbf{d}_k\right) \\
    Z(\mathbf{x}) & = \sum_{\mathbf{\tilde{y}}\in Val(\mathbf{Y})}\tilde{P}(\mathbf{\tilde{y}},\mathbf{x})
  \end{split}
\end{equation}
where, similar to \glspl{gibbs distribution} (see \Cref{equ:gibbs-distribution}), $\tilde{P}(\mathbf{y},\mathbf{x})$ is the unnormalized measure and $Z(\mathbf{X})$ is a normalizing constant~\citep{koller2009probabilistic}.
Additionally we have that $\mathbf{d}_k=\mathbf{\tilde{x}}\cup\mathbf{\tilde{y}}$ and $\mathbf{d}_k\not\subseteq\mathbf{x}$ where $\mathbf{\tilde{x}}\in Val(\mathbf{X})$.\todo{rewrite}
In other words, $\mathbf{d}_k$ needs to contain at least one $y_n\in Val(\mathbf{Y})$.
In \todo{add ref and example} we use an example to show that in the case of $\mathbf{d}_k\subseteq Val(\mathbf{X})$, the term $\Psi_k(\mathbf{d}_k)$ ``cancels out'' during the calculation of $P(\mathbf{y}\mid\mathbf{x})$.

This behavior for a $\mathbf{d}_k\subseteq\mathbf{X}$ is the result of the only difference between the definition of a \gls{gibbs distribution} in \Cref{equ:gibbs-distribution} and the definition of a \gls{crf} above, namely how the normalizing constant $Z$ is defined~\citep{koller2009probabilistic}.
In \Cref{equ:gibbs-distribution}, $Z$ normalizes $\tilde{P}(x_1,\dots,x_N)$ with a sum over all possible assignments to $X_1,\dots,X_N$ resulting in a \gls{joint distribution} $P(x_1,\dots,x_N)$\todo{rewrite probs.}.
In \Cref{equ:crf-factor}, however, $Z(\mathbf{x})$ normalizes $\tilde{P}(\mathbf{y},\mathbf{x})$ with a sum over the assignments $\mathbf{\tilde{y}}$ to $\mathbf{Y}$ over the observed assignments $\mathbf{x}$.
This way of normalizing results in the \gls{conditional probability distribution} $P(\mathbf{y}\mid\mathbf{x})$ (Compare with \Cref{equ:conditional-probability}).

\bigskip

Using \Cref{equ:log-linear-model}, we can reformulate the definition of \glspl{crf} in \Cref{equ:crf-factor} as a \gls{log-linear model}:
\begin{equation}
  \label{equ:crf-log-linear}
  \begin{split}
    P(\mathbf{y}\mid\mathbf{x}) & = \frac{1}{Z(\mathbf{x})}\tilde{P}(\mathbf{y},\mathbf{x}) \\
    \tilde{P}(\mathbf{y},\mathbf{x}) & = \exp\left\{ -\sum_{k=1}^K \theta_k f_k\left(\mathbf{d}_k\right)\right\} \\
    Z(\mathbf{x}) & = \sum_{\mathbf{\tilde{y}}\in Val(\mathbf{Y})}\tilde{P}(\mathbf{\tilde{y}},\mathbf{x})
  \end{split}
\end{equation}
This representation allows us to encode a \gls{crf} model more compactly using \glspl{feature function} instead of \glspl{factor}.
This will become more clear in the following discussion.

\bigskip

A specific kind of \gls{crf} that follows a rather simple structure are \glspl{linear-chain crf}.
For given $\mathbf{X}=\{X_1,\dots,X_N\}$ and $\mathbf{Y}=\{Y_1,\dots,Y_N\}$, we consider two types of \glspl{factor}:
\begin{itemize}
  \item $\Psi_n(y_n,y_{n-1})$ models for given assignments $y_n$ and $y_{n-1}$ the dependency between their corresponding \gls{target variable} $Y_n$ and its preceding \gls{target variable} $Y_{n-1}$.
  \item $\Psi_n(y_n,\mathbf{\tilde{x}}_n)$ models, for the given assignment to the \gls{target variable} $Y_n=y_n$ and assignments to the observed variables $\mathbf{\tilde{X}}_n=\mathbf{\tilde{x}}_n$, the dependency between the \gls{target variable} and its context given by the \glspl{observed variable} $\mathbf{\tilde{X}}_n=\{\tilde{X}_1,\dots,\tilde{X}_T\}$ with $\mathbf{\tilde{X}}_n\subseteq\mathbf{X}$.
    $T$ can be different for every $\mathbf{\tilde{X}}_n$.
\end{itemize}
Inserting the two factors in \Cref{equ:crf-factor} results in:
\begin{equation}
  \label{equ:linear-chain-crf-factor}
  \begin{split}
    P(\mathbf{y}\mid\mathbf{x}) & = \frac{1}{Z(\mathbf{x})}\tilde{P}(\mathbf{y},\mathbf{x}) \\
    \tilde{P}(\mathbf{y},\mathbf{x}) &= \prod_{n=1}^{N}\left(\Psi_n\left(\vphantom{\tilde{x}}y_n,y_{n-1}\right)\times\Psi_n\left(y_n,\mathbf{\tilde{x}}_n\right)\right) \\
    Z(\mathbf{x}) & = \sum_{\mathbf{\tilde{y}}\in\mathbf{Y}}\tilde{P}(\mathbf{\tilde{y}},\mathbf{x})
  \end{split}
\end{equation}
We now can represent the two \glspl{factor} using the \glspl{feature function}
\begin{equation}
  \label{equ:linear-chain-crf-feature-functions}
  \begin{split}
    \tilde{f}_k\left(\vphantom{\tilde{x}}y_n,y_{n-1}\right)\equalsdef f_{j,i}\left(\vphantom{\tilde{x}}y_n,y_{n-1}\right) & = \mathbf{\mathbbm{1}}\left\{\vphantom{\tilde{x}}y_n=j,y_{n-1}=i\right\}\\
    \tilde{f}_l\left(y_n,\mathbf{\tilde{x}}_n\right)\equalsdef f_{j,\bm{i}}\left( y_n,\mathbf{\tilde{x}}_n\right) & = \mathbf{\mathbbm{1}}\left\{y_n=j,\mathbf{\tilde{x}}_n=\bm{i}\right\}
  \end{split}
\end{equation}
where $i$ and $j$ are predefined values and $\bm{i}$ is a set of predefined values with $|\bm{i}|=|\mathbf{\tilde{x}}_n|$. $\mathbf{\mathbbm{1}}$ is an indicator function that returns $1$ if all listed assignments match the predefined values.
The $\tilde{f}$ notation with its indices will later allow us to iterate over the predefined \glspl{feature function} in a more compact way.

In other words, $\tilde{f}_k(y_n,y_{n-1})$ models a specific dependency between the neighboring \glspl{target variable} $Y_n$ and $Y_{n-1}$. $\tilde{f}_l(y_n,\mathbf{\tilde{x}}_n)$ models a specific dependencies between the \gls{target variable} $Y_n$ and a set of \glspl{observed variable} $\mathbf{\tilde{X}}_n$ in context of $Y_n$ where again $\mathbf{\tilde{X}}_n\subseteq\mathbf{X}$.

By inserting the two types of \glspl{feature function} from \Cref{equ:linear-chain-crf-feature-functions} into \Cref{equ:crf-log-linear} we define \glspl{linear-chain crf} as:
\begin{equation}
  \label{equ:linear-chain-crf-log-linear}
  \begin{split}
    P(\mathbf{y}\mid\mathbf{x}) & = \frac{1}{Z(\mathbf{x})}\tilde{P}(\mathbf{y},\mathbf{x})  \\
    \tilde{P}(\mathbf{y},\mathbf{x}) & = \exp\left\{ -\sum_{n=1}^N \left(\sum_{k=1}^K\theta_k \tilde{f}_k\left(\vphantom{\tilde{x}}y_n,y_{n-1}\right)+\sum_{l=1}^L\theta_l \tilde{f}_l\left(y_n,\mathbf{\tilde{x}}_n\right)\right) \right\} \\
    Z(\mathbf{x}) & = \sum_{\mathbf{\tilde{y}}\in\mathbf{Y}}\tilde{P}(\mathbf{\tilde{y}},\mathbf{x})
  \end{split}
\end{equation}
Thereby, for every $y_n$, we iterate over $K$ predefined \glspl{feature function} $\tilde{f}_k$ and $L$ predefined \glspl{feature function} $\tilde{f}_l$.
Additionally we have a set of parameters $\bm{\theta}$ which are multiplied with the two different feature functions.
$\bm{\theta}$ thereby contains $K+L$ parameters.
In order to simplify the notation, we define $y_0$ as a special start state, denoted \texttt{start}~\citep{lafferty2001conditional}.

An example for a  \gls{linear-chain crf} can be derived from our author example\dots\todo{example}

Returning to our argument that \glspl{feature function} allow are move compact encoding of \glspl{crf} than \glspl{factor}, compare \todo{example}.

\citet{sutton2010introduction} show that \glspl{hmm} are a restricted kind of \gls{linear-chain crf}, namely one where the set of assignments $\mathbf{\tilde{x}}_n$ in \Cref{equ:linear-chain-crf-log-linear} only includes one \gls{observed variable} $x_n$ which corresponds to the words identity at the current position $n$.

\section{Inference of \glsentryshortpl{crf}}\label{sec:inference-crfs}

After formalizing the encoding of \glspl{crf} we can now discuss their inference.
Here we consider the inferencing task for \glspl{probability query} $P(\mathbf{Y}|\mathbf{X}=\mathbf{x})$ (see \Cref{subsec:probability-theory}).
\citet{sutton2010introduction} distinguish two kinds of inference problems:
\begin{enumerate}
  \item Given a trained model and observed assignments $\mathbf{X}=\mathbf{x}$, predict the most likely assignment $\mathbf{Y}=\mathbf{\hat{y}}$:
    \begin{equation}
      \label{equ:inference-argmax}
      \argmax\limits_{\mathbf{\hat{y}}} P(\mathbf{Y}=\mathbf{\hat{y}}\mid\mathbf{X}=\mathbf{x})
    \end{equation}
  \item During the parameter estimation during learning, compute the \gls{marginal distribution} for \dots\todo{add based on learning chapter}
\end{enumerate}
%TODO about that 1. and 2. are same fundamental operations (sutton2010, p. 27)
By replacing the summation operator over $\mathbf{y}$ in the marginalization in the second case by the $\argmax$ function, we effectively result in the prediction problem in the first case.
Thereby, the two kinds of inference problems can be seen as fundamentally the same operation~\citep{sutton2010introduction}.
In the following we will focus on the inference of the normalizing constant $Z(\mathbf{x})$ in the context of \glspl{linear-chain crf}.
An approach to the prediction task can be derived from it.

\bigskip

In order to show the computational complexity of this inferencing task, we will consider \glspl{linear-chain crf} on a \gls{factor} level.
According to \Cref{equ:linear-chain-crf-factor} we have:
\begin{equation}
  \label{equ:linear-chain-crf-z}
  Z(\mathbf{x}) = \sum_{\mathbf{\tilde{y}}\in\mathbf{Y}}\left(\prod_{n=1}^{N}\left(\Psi_n\left(\vphantom{\tilde{x}}y_n,y_{n-1}\right)\times\Psi_n\left(y_n,\mathbf{\tilde{x}}_n\right)\right)\right)
\end{equation}
Since we calculate the \gls{factor product} for every possible joint assignment $\mathbf{\tilde{y}}\in Val(\mathbf{Y})$, the computation of $Z(\mathbf{x})$ is exponential \dots
\itodo{show complexity with example?}

Yet, in the case of \glspl{linear-chain crf}, it is possible to reduce the complexity of calculating $Z(\mathbf{x})$ by using the forward-backward algorithm, sometimes referred to as dynamic programming or variable elimination~\citep{sutton2010introduction,koller2009probabilistic}.
The key insight of the forward-backward algorithm is that during the na\"{\i}ve calculation of, in our case, $Z(\mathbf{x})$, partial calculations are repeated exponentially often.
In order to prevent unnecessary repetitions of the same calculations, intermediate results are stored and reused.
The following demonstration is based on the calculation of $P(\mathbf{x})$ for \glspl{hmm} using the forward-backward algorithm in \citet{sutton2010introduction}.

\bigskip

By applying the distributive law we can reorder \Cref{equ:linear-chain-crf-z} in the following way:
\begin{equation}
  \label{equ:linear-chain-crf-z-distributive}
  \begin{split}
    Z(\mathbf{x}) = & \sum_{y_N}\left(\sum_{y_{N-1}}\left(\Psi_N\left(\vphantom{\tilde{x}}y_N,y_{N-1}\right)\times\Psi_N\left(y_N,\mathbf{\tilde{x}}_N\right)\right)\left(\sum_{y_{N-2}}\left(\Psi_{N-1}\left(\vphantom{\tilde{x}}y_{N-1},y_{N-2}\right)\right.\right.\right.\\
    & \left.\left.\left.\times\Psi_{N-1}\left(y_{N-1},\mathbf{\tilde{x}}_{N-1}\right)\right)\dots\vphantom{\sum_{y_N}}\right)\right)
  \end{split}
\end{equation}
As we can see, the inner sums are needed multiple times by the outer sums.
In a na\"{\i}ve approach, these inner sums would be recalculated every time they appear in an outer sum.\todo{example?}
Instead, our goal is to calculate the inner sums once and store their result for later usage.

\bigskip

In order to find a recursive definition of $Z(\mathbf{x})$ that allows the reuse of intermediate calculations, we first consider the following case
\begin{subequations}
  \begin{equation}
    \label{equ:linear-chain-crf-z-j-1}
    \begin{split}
      Z(\mathbf{x},y_N=j) = & \sum_{y_1,\dots y_{N-1}}\left(\vphantom{\prod_{t'}^N}\left(\Psi_N\left(\vphantom{\tilde{x}}j,y_{N-1}\right)\times\Psi_N\left(j,\mathbf{\tilde{x}}_N\right)\right)\right. \\
      & \left.\times\prod_{t'=1}^{N-1}\left(\Psi_{t'}\left(\vphantom{\tilde{x}}y_{t'},y_{t'-1}\right)\times\Psi_{t'}\left(y_{t'},\mathbf{\tilde{x}}_{t'}\right)\right)\right)
    \end{split}
  \end{equation}
  where we have $y_N=j$ and where the calculations including $j$ are isolated from the product over all other calculations.

  Since in the two \glspl{factor} $\Psi_N$ we only use the assignments $y_{N-1}$ from the sum over $y_1,\dots,y_{n-1}$, we can rewrite \Cref{equ:linear-chain-crf-z-j-1} as
  \begin{equation}
    \label{equ:linear-chain-crf-z-j-2}
    \begin{split}
      Z\left(\mathbf{x},y_N=j\right) = & \sum_{y_{N-1}}\left(\vphantom{\prod_{t'_N}^N}\left(\Psi_N\left(\vphantom{\tilde{x}}j,Y_{N-1}\right)\times\Psi_N\left(j,\mathbf{\tilde{x}}_N\right)\right)\right. \\
      &\left. \times\sum_{y_1,\dots,y_{N-1}}\prod_{t'=1}^{N-1}\left(\Psi_{t'}\left(\vphantom{\tilde{x}}y_{t'},y_{t'-1}\right)\times\Psi_{t'}\left(y_{t'},\mathbf{\tilde{x}}_{t'}\right)\right)\right)
    \end{split}
  \end{equation}
  and using the definition of $Z(\mathbf{x})$ in \Cref{equ:linear-chain-crf-z} we arrive at
  \begin{equation}
  \label{equ:linear-chain-crf-z-j-3}
  Z\left(\mathbf{x},y_N=j\right) = \sum_{y_{N-1}}\left(\left(\Psi_N\left(\vphantom{\tilde{x}}j,y_{N-1}\right)\times\Psi_N\left(j,\mathbf{\tilde{x}}_N\right)\right)\times Z\left(x_1,\dots,x_{N-1}\right)\right).
  \end{equation}
\end{subequations}
Generalizing from this case we can now define
\begin{equation}
  \label{equ:linear-chain-crf-z-alpha}
  \alpha_n(j) = \sum_{i\in\mathbf{y}}\left(\left(\Psi_n\left(\vphantom{\tilde{x}}j,i\right)\times\Psi_n\left(j,\mathbf{\tilde{x}}_n\right)\right)\times \alpha_{n-1}(i)\right)
\end{equation}
where the position of assignment $i$ in $\mathbf{y}$ is derived from the definition $\Psi_n(j,i)$.
Thereby, if $Y_n=j$ then $i$ is an assignment to $Y_{n-1}$.
The base case for the recursion is defined as
\begin{equation}
  \label{equ:linear-chain-crf-z-alpha-base}
 \alpha_1(j) = \Psi_1\left(\vphantom{\tilde{x}}j,y_0\right)\times\Psi_1\left(j,\mathbf{\tilde{x}}_1\right)
\end{equation}
where $y_0$ again is the \texttt{start} state. Since the calculation is recursive in a way that $\alpha_n$ is calculated using the result of $\alpha_{n-1}$, we refer to $\alpha_n$ as a \textit{forward variable}~\citep{sutton2010introduction}. In order to calculate $Z(\mathbf{x})$ using $\alpha_n(j)$, we calculate the sum over all assignments $\mathbf{\tilde{y}}\in Val(\mathbf{Y})$, resulting in
\begin{equation}
  \label{equ:linear-chain-crf-z-alpha-sum}
  Z(\mathbf{x})=\sum_{\mathbf{\tilde{y}}\in Val(\mathbf{Y})}\alpha_T\left(\mathbf{\tilde{y}}\right)
\end{equation}
where $T=|\mathbf{\tilde{y}}|$.
\bigskip

Even though the forward variable $\alpha_n$ is sufficient for calculating $Z(\mathbf{x})$, other calculations may also require a \textit{backward variable} $\beta_n$. It is defined analogously to $\alpha_n$ but the recursion expands in the opposite direction such that the calculation of $\beta_n$ recursively uses the result of $\beta_{n+1}$:
\begin{equation}
  \label{equ:linear-chain-crf-z-beta}
  \beta_n(i) = \sum_{j\in\mathbf{y}}\left(\left(\Psi_{n+1}\left(\vphantom{\tilde{x}}j,i\right)\times\Psi_{n+1}\left(j,\mathbf{\tilde{x}}_{n+1}\right)\right)\times \beta_{n+1}(j)\right)
\end{equation}
Due to the opposite order, the base case for the recursion is defined as $\beta_N(i)=1$.
Also, note the inverted usage of $i$ in $\beta_n(i)$ and $j$ in $\beta_{n+1}(j)$ in comparison with the usage in \Cref{equ:linear-chain-crf-z-alpha} for $\alpha_n$.
It is also possible to calculate $Z(\mathbf{x})$ using $\beta_n$ with
\begin{equation}
  \label{equ:linear-chain-crf-z-beta-sum}
  Z(\mathbf{x})=\beta_0\left(y_0\right)\equalsdef\sum_{y_1}\left(\left(\Psi_1\left(\vphantom{\tilde{x}}y_1,y_0\right)\times\Psi_1\left(y_1,\mathbf{\tilde{x}}_1\right)\right)\times\beta_1\left(y_1\right)\right)
\end{equation}
in order to correctly handle the \texttt{start} state $Y_0$.
Again, the definitions of $\alpha_n$ and $\beta_n$ are derived from \citet{sutton2010introduction} and their example for \glspl{hmm}.

\bigskip

By using both $\alpha_n$ and $\beta_n$ it is possible to approach a calculation from both sides of a sequence\todo{example for $\tilde{P}(\mathbf{Y},\mathbf{X})$, sutton2010 p.34 top}

%TODO few words on Viterbi algorithm (sutton2010)

%TODO exact inference (variable elimination/conditioning?) vs approximate inference (message-passing/optimization problem/particle-based methods?)
\bigskip

When applying the forward-backward algorithm to \glspl{linear-chain crf}, we exploit their sequential structure.
Yet, not all \gls{crf} models follow such a structure and thereby we can not always apply this algorithm.
A number of alternative algorithms for both exact and approximate inference for such general graphs exist.
\itodo{list algorithms and link them}
Since our approach on author extraction uses \glspl{linear-chain crf}, we will not further discuss these alternatives.
Instead, we refer to \citet{koller2009probabilistic} for further readings on this topic.

\bigskip

In the following section we will discuss methods for learning \glspl{crf}.
In particular we will look into the parameter estimation of the set of weights $\bm{\theta}$ that are assigned to \glspl{feature function}, for example in \Cref{equ:linear-chain-crf-log-linear}.

\section{Learning of \glsentryshortpl{crf}}\label{sec:learning-crfs}

After discussing the encoding and inference of \glspl{crf}, we now look into their learning.
The main focus of this section will be on the learning of the parameters $\theta$ in the \gls{log-linear model} representation of \glspl{crf}, such as the one in \Cref{equ:linear-chain-crf-log-linear}.

\bigskip

Constructing \gls{crf} models manually is time costly and requires domain knowledge.
To acquire such knowledge, often experts from this domain need to be involved.
In some cases, experts with a sufficient understanding of the domain do not exist~\citep{koller2009probabilistic}.
Yet, we now often have access to a large body of example instances which originate from the distribution we want to model~\citep{koller2009probabilistic}.
A promising approach is thereby to learn a model $\mathcal{\tilde{M}}$ based on such a set of examples.
More formally, we assume a given labeled data set $\mathcal{D}=\{\mathpzc{d}^{(1)},\dots,\mathpzc{d}^{(M)}\}$ of $M$ instances with $\mathpzc{d}^{(m)}=\mathbf{x}^{(m)}\cup\mathbf{y}^{(m)}$ and $\mathbf{X}^{(m)}\cap\mathbf{Y}^{(m)}=\emptyset$.
We further assume that $\mathcal{D}$ follows an underlying distribution $P^*$ which is induced by a network $\mathcal{M}^*=(\mathcal{K}^*,\bm{\theta}^*)$ where $\mathcal{K}^*$ is a graph and $\bm{\theta}^*$ are model parameters~\citep{koller2009probabilistic}.
Lastly, we assume that the elements in $\mathcal{D}$ sampled independently from $P^*$ and thereby \acrfull{iid}~\citep{koller2009probabilistic}.

\bigskip

Learning a model $\mathcal{\tilde{M}}$ that exactly induces $P^*$ is often unfeasible in practice due to computational limitations and since $\mathcal{D}$ only provide an approximation of $P^*$~\citep{koller2009probabilistic}.
Instead, the goal is to find a $\mathcal{\tilde{M}}$ which provides the ``best'' approximation to $\mathcal{M}^*$ given our $\mathcal{D}$.
There exists a number of metrics for deciding which $\mathcal{\tilde{M}}$ ``best'' approximates $\mathcal{M}^*$.

A popular metric that is used for \glspl{crf} is \gls{maximum likelihood}.
Before discussing it, it is important to clarify which part of $\mathcal{\tilde{M}}=(\mathcal{\tilde{K}},\bm{\tilde{\theta}})$ we aim to learn.
%TODO add forward-backward algorithm to gls
In order to apply the forward-backward algorithm during inferencing, which is also part of the learning, $\mathcal{\tilde{K}}$ needs to follow a certain sequential structure.
In this work we assume $\mathcal{\tilde{K}}$ to be modeled in such a way and given as an input to the learner.
A relatively simple model for $\mathcal{\tilde{K}}$ can be a log-linear \glspl{linear-chain crf} (see \Cref{equ:linear-chain-crf-log-linear}). For it we have a \gls{feature function} $\tilde{f}_k$ between two neighboring \glspl{target variable} and a \gls{feature function} $\tilde{f}_l$ between a \gls{target variable} and its context given by a set of \glspl{observed variable}.
Thereby, we will focus on the learning of the set of parameters $\bm{\tilde{\theta}}$ which, in the case of \glspl{linear-chain crf}, are the $\theta_k$ and $\theta_l$ for the two \glspl{feature function} from \Cref{equ:linear-chain-crf-log-linear}.

\bigskip
Given a data set $\mathcal{D}$ we define the likelihood function $L(\bm{\tilde{\theta}}:\mathcal{D})$ as
\begin{equation}
  \label{equ:likelihood}
  \mathcal{L}\left(\bm{\tilde{\theta}}:\mathcal{D}\right)=\prod_{m=1}^M P\left(\mathbf{y}^{(m)}|\mathbf{x}^{(m)}\right)
\end{equation}
where $P$ is derived from the graphical model $\mathcal{\tilde{M}}=(\mathcal{\tilde{K}},\bm{\tilde{\theta}})$.
A common approach is to calculate the \textit{log-likelihood} $\ell$ instead of the likelihood itself.
This is possible since the log-likelihood is monotonically related to the likelihood and thereby the maximization problems are equivalent~\citep{koller2009probabilistic}.
An computational advantage of using the log-likelihood is that it is calculated over sums instead of products~\citep{koller2009probabilistic}.

The log-likelihood function is defined as~\citep{sutton2010introduction}:
\begin{equation}
  \label{equ:log-likelihood}
  \ell\left(\bm{\tilde{\theta}}:\mathcal{D}\right)=\sum_{m=1}^M \left(\log P\left(\mathbf{y}^{(m)}|\mathbf{x}^{(m)}\right)\right)
\end{equation}

In order to illustrate the calculation, we now look at a more concrete example of a log-likelihood function.
Using the definition of log-linear \glspl{linear-chain crf} from \Cref{equ:linear-chain-crf-log-linear}, we now substitute $P(\mathbf{y}^{(m)}|\mathbf{x}^{(m)})$ in \Cref{equ:log-likelihood} which gives us:
\begin{subequations}
\begin{equation}
  \label{equ:log-likelihood-linear-chain-crf-log-linear-1}
  \begin{split}
    \ell\left(\bm{\tilde{\theta}}:\mathcal{D}\right) = & \sum_{m=1}^M \left(\log \left(\frac{1}{Z\left(\mathbf{x}^{(m)}\right)}\exp\left\{ -\sum_{n=1}^N \left(\sum_{k=1}^K\theta_k \tilde{f}_k\left(y_n^{(m)},y_{n-1}^{(m)}\right) \right.\right.\right.\right.\\
    &\left.\left.\left.\left. +\sum_{l=1}^L\theta_l \tilde{f}_l\left(y_n^{(m)},\mathbf{\tilde{x}}_n^{(m)}\right)\right)\right\}\right)\right)
 \end{split}
\end{equation}
We can simplify this by resolving the $\log$ statement which results in:
\begin{equation}
  \label{equ:log-likelihood-linear-chain-crf-log-linear-2}
  \begin{split}
    \ell\left(\bm{\tilde{\theta}}:\mathcal{D}\right) = & \sum_{m=1}^M \left(-\sum_{n=1}^N \left(\sum_{k=1}^K\theta_k \tilde{f}_k\left(y_n^{(m)},y_{n-1}^{(m)}\right)+\sum_{l=1}^L\theta_l \tilde{f}_l\left(y_n^{(m)},\mathbf{\tilde{x}}_n^{(m)}\right)\right)\right) \\
    & -\log Z\left(\mathbf{x}^{(m)}\right)
 \end{split}
\end{equation}
\end{subequations}
Using the forward-backward algorithm we can efficiently calculate the normalization constant $Z(\mathbf{x}^{(m)})$ as shown in \Cref{sec:inference-crfs}.

\bigskip

\citet{sutton2010introduction} discuss that a model $\tilde{\mathcal{M}}$ can overfit the given data set $\mathcal{D}$ when the number of parameters in $\bm{\tilde{\theta}}$ is too high.
It is thereby common to use a \textit{regularization term} that forms a penalty based on the norm of $\bm{\tilde{\theta}}$ where $\bm{\tilde{\theta}}$ is seen as a vector~\citep{koller2009probabilistic,sutton2010introduction}.

An example for such a term is called \textit{Gaussian prior}.
\itodo{understand and explain Gaussian prior}
\begin{equation}
  \label{equ:gaussian-prior}
  Gauss(\bm{\theta})=\exp\left\{\sum_{i=1}^I\frac{\theta_i^2}{2\sigma^2}\right\}
\end{equation}
\begin{equation}
  \label{equ:gaussian-prior}
  Gauss(\bm{\theta})=\sum_{i=1}^I\frac{\theta_i^2}{2\sigma^2}
\end{equation}
%TODO discuss with Rene

\begin{equation}
  \label{equ:log-likelihood-linear-chain-crf-log-linear-gaussian}
  \begin{split}
    \ell\left(\bm{\tilde{\theta}}:\mathcal{D}\right) = & \sum_{m=1}^M \left(-\sum_{n=1}^N \left(\sum_{k=1}^K\theta_k \tilde{f}_k\left(\vphantom{\tilde{x}}y_n^{(m)},y_{n-1}^{(m)}\right)+\sum_{l=1}^L\theta_l \tilde{f}_l\left(y_n^{(m)},\mathbf{\tilde{x}}_n^{(m)}\right)\right)\right) \\
    & -\log Z\left(\mathbf{x}^{(m)}\right)-\left(\sum_{k=1}^K\frac{\theta_k^2}{2\sigma^2}+\sum_{l=1}^L\frac{\theta_l^2}{2\sigma^2}\right)
 \end{split}
\end{equation}
In \Cref{cha:distant-supervision} we discuss an additional regularization term based on the concept of \gls{generalized expectation}.

\bigskip

After discussing a concrete calculation of $\ell(\bm{\tilde{\theta}}:\mathcal{D})$ we now want to find the set of parameters $\bm{\hat{\theta}}\in\mathbf{\Theta}$ such that $\ell(\bm{\hat{\theta}}:\mathcal{D})$ has the maximum value regarding a predefined graph $\mathcal{\tilde{K}}$.
Here, $\bm{\Theta}$ is the set of all possible sets of parameters $\bm{\tilde{\theta}}$.
This task is referred to as \textit{maximum likelihood estimation} and, when considering the log-likelihood function, defined as~\citep{koller2009probabilistic}:
\begin{equation}
  \label{equ:maximum-log-likelihood-estimation}
  \ell\left(\bm{\hat{\theta}}:\mathcal{D}\right)=\max_{\bm{\tilde{\theta}}\in\mathbf{\Theta}}\ell\left(\bm{\tilde{\theta}}:\mathcal{D}\right)
\end{equation}
When discussing the maximization problem, an important term is the \gls{gradient} of a \gls{function}.
The \gls{gradient} $\nabla f$ of an objective function\todo{explain obj.\ function} $f_{\text{obj}}(\bm{\tilde{\theta}})$ with $\bm{\tilde{\theta}}=\tilde{\theta}_1,\dots,\tilde{\theta}_I$ is the vector of the partial derivatives~\citep{koller2009probabilistic}:
\begin{equation}
  \label{equ:gradient}
  \nabla f=\left\langle\frac{\partial f}{\partial\tilde{\theta}_1},\dots,\frac{\partial f}{\partial\tilde{\theta}_I}\right\rangle
\end{equation}
The first step in finding $\bm{\hat{\theta}}$ is to find a $\bm{\tilde{\theta}}$ for which $\nabla f=0$.
Applied to our log-likelihood function we thereby have to solve the following equations:
\begin{equation}
  \label{equ:log-likelihood-gradient}
  \frac{\partial}{\partial\theta_i}\ell\left(\bm{\tilde{\theta}}:\mathcal{D}\right)=0\ \ \ \ \ \ \ \ \ i=1,\dots,I
\end{equation}
We refer to \citet{sutton2010introduction} and \citet{koller2009probabilistic} for further information on how to calculate these partial derivatives.
The resulting $\bm{\tilde{\theta}}$ is called a \textit{stationary point} of the function $\ell$ and can be a local maximum, a local minimum, or a saddle point~\citep{koller2009probabilistic}.
There are multiple ways of controlling if $\bm{\tilde{\theta}}$ is a local maximum, for example by checking the second derivative of $\ell(\bm{\tilde{\theta}}:\mathcal{D})$.
If it is negative then $\bm{\tilde{\theta}}$ is a local maximum~\citep{koller2009probabilistic}.

\citep{sutton2010introduction} argue that, in the case of \glspl{linear-chain crf}, the function $\ell(\bm{\tilde{\theta}}:\mathcal{D})$ (see \Cref{equ:log-likelihood-linear-chain-crf-log-linear-gaussian}) is concave.
Thereby, during the optimization of $\ell$, every local optimum is also a global optimum~\citep{sutton2010introduction}.

\bigskip

A typical approach to the optimization uses gradient ascent methods~\citep{koller2009probabilistic}.
Starting with an arbitrary $\bm{\tilde{\theta}}$, the goal is to follow the slope of $\bm{\tilde{\theta}}$ by iteratively modifying the $\bm{\tilde{\theta}}$ and recalculating the gradient $\nabla f$.
This is done until the maximum is reached.
Yet, since this approach requires many calculations of $\nabla f$, it can be infeasible in practice~\citep{sutton2010introduction}.

A number of improvements to this have been made that aim to reduce the number of such calculations.
This is often done by taking into account information from the second derivative of the objective function~\citep{sutton2010introduction}.
Since the matrix of all second derivatives, called the \textit{Hessian}, is quadratic in the number of parameters, computing the full \textit{Hessian} can again be infeasible~\citep{sutton2010introduction}.
By approximating the Hessian with the \gls{bfgs} algorithm and limiting its memory requirements, \citet{byrd1994representations} provide an approach, called \textit{L-BFGS}, to the maximization problem that can handle a large number of parameters.
\citet{andrew2007scalable} state that L-BFGS ``is the algorithm of choice for optimizing the parameters of large-scale log-linear models with $L_2$ regularization''~\citep{andrew2007scalable}.
$L_2$ regularization is an alternative to the Gaussian regularization (see \todo{add ref}).

\bigskip

After giving an introduction to the encoding, inferencing, and learning of \glspl{crf} we will discuss in the following chapter how to incooperate \gls{distant supervision} in the \gls{crf} learning process.
