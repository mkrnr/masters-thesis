\chapter{Conditional Random Fields (\glsentryshortpl{crf})}\label{cha:crfs}

As we discussed in the previous chapter, \glspl{crf} are widely used in the related work for learning probabilistic models on given data.
In this chapter we give an introduction to this framework.
First we provide an overview of relevant concepts in probability theory, graph theory, and graphical models.
In addition to relevant definitions we will use a simplified example (see \todo{ref to Appendix}) that is based on the extraction of author information from reference strings in research papers.
Following this we will introduce the concept of \glspl{crf} and discuss the inference and learning of \gls{crf} models for the task of entity recognition.

\section{Foundations}\label{sec:foundations}
\subsection{Probability Theory}\label{subsec:probability-theory}
Several concepts from probability theory are crucial for an understanding of \glspl{crf}.
The notion of \glspl{probability distribution} is one such concept.
A \gls{probability distribution} $P$ is defined over $(\glssymbol{outcome space},\glssymbol{measurable set})$ where \glssymbol{outcome space} is the \gls{outcome space} and \glssymbol{measurable set} is a \gls{measurable set} of \glspl{event} \citep{koller2009probabilistic}.
$P$ describes a mapping from events in \glssymbol{measurable set} to real values according to the following rules \citep{koller2009probabilistic}:
\begin{itemize}
  \item $P(\alpha)\geq 0 $ for all $ \alpha \in S$.
  \item $P(\glssymbol{outcome space})=1$.
  \item If $\alpha,\beta\in \glssymbol{outcome space}$ and $\alpha\cap\beta = \emptyset$, then $P(\alpha\cup\beta)=P(\alpha)+P(\beta)$.
\end{itemize}

In our example of author information extraction we define \glssymbol{outcome space} as the set of possible word sequences that can appear in a given text line of a research paper.
For the purpose of illustration we consider four events:

Event $firstPeriod$ contains all word sequences in which the first word of the sequence ends with a period and event $firstNotPeriod$ contains $\Omega\ -\ firstPeriod$.
Event $secondCapitalized$ contains all word sequences in which the second word of the sequence is capitalized and event $secondNotCapitalized$ is defined accordingly.

\glssymbol{measurable set} contains the four defined events and per definition also $\emptyset$ and $\Omega$.
$P$ now assigns all events in \glssymbol{measurable set} a probability.
Given the values in \todo{add ref}, $P(firstPeriod)$ is $XXX$ and $P(secondNotCapitalized)$ is $XXX$.

\bigskip

Based on the idea of \glspl{probability distribution}, a \gls{random variable} is a \gls{function} that associates with each outcome in \glssymbol{outcome space} a value~\cite{koller2009probabilistic}.
A set of \glspl{random variable} is denoted with bold capital letters and assignments of values to the \glspl{random variable} in such sets are denoted with bold lowercase letters~\cite{koller2009probabilistic}.
In addition, $Val(X)$ is the set of values that $X$ can take.
Given a \gls{random variable} $X$, $P(X)$ is the distribution over \glspl{event} that are described using $X$~\cite{koller2009probabilistic}.
It is referred to as the \gls{marginal distribution} over $X$.
Given a set of \glspl{random variable} $\bm{X}$, $P(\bm{X})$ is called \gls{joint distribution} and assigns each \gls{full assignment} to $\bm{X}$ a probability~\cite{koller2009probabilistic}.
A \gls{full assignment} $\xi$ assigns to every \gls{random variable} in $\bm{X}$ a value and thereby $\xi\in Val(\bm{X})$.
Any \gls{event} that is described using $\bm{X}$ must be a union of events that correspond to \glspl{full assignment} to $\bm{X}$.
This gives us a \gls{canonical outcome space} where each outcome is a joint assignment to the \glspl{random variable} in $\bm{X}$.

Following our example we define a \gls{random variable} $Y$ as the \gls{function} that associates with the first word in a sequence whether or not it end with a period.
Thereby, the \gls{event} $firstPeriod$ can now also be denoted with $Y = period$ and $firstNotPeriod$ with $Y = notPeriod$ resulting in $Val(Y)=\{period, notPeriod\}$.
Consequently, $P(firstPeriod)=P(Y=period)$ and $P(firstNotPeriod)=P(Y=period)$.
$X$ is defined as the function that associates with the second word in a sequence whether or not it is capitalized with $Val(X)=\{capitalized, notCapitalized\}$.
Given $X$ and $Y$, we can build the \gls{joint distribution} $P(X,Y)$.
Using the values from \todo{ref}, $P(X=capitalized,Y=period)=XXX$ and $P(X=capitalized,Y=notPeriod)=XXX$.

\bigskip

The \gls{conditional probability} of an event $\alpha$ given $\beta$ with $P(\beta)>0$ is defined as~\cite{koller2009probabilistic}:

\begin{equation}
P(\alpha\mid\beta) = \frac{P(\alpha\cap\beta)}{P(\beta)}
\label{equ:conditional-probability}
\end{equation}

This definition can be extended to \glspl{random variable}.
Given that all assignments in $Y$ are non-zero, the \gls{conditional probability distribution} $P(X\mid Y)$ assigns each value $Y$ a probability over values of $X$ using the \gls{conditional probability}.
This can also be extended to sets of \glspl{random variable}.

With our example values from \todo{ref}, $P(X=capitalized\ \mid\ Y=period)=xxx$ and $P(X=capitalized\mid Y=notPeriod)=xxx$.

\bigskip

Another relevant concept is the notion of \gls{independence}.
An \gls{event} $\alpha$ is \glslink{independence}{independent} of \gls{event} $\beta$ in $P$ if $P(\alpha\mid\beta)=P(\alpha)$ or if $P(\beta)=0$~\cite{koller2009probabilistic}.
The \gls{independence} of $\alpha$ and $\beta$ is denoted with $P\models(\alpha\perp\beta)$.
Two \glspl{event} can also be \glslink{independence}{independent} given a third \gls{event}, called \gls{conditional independence}:
$\alpha$ is \glslink{conditional independence}{conditionally independent} of \gls{event} $\beta$ given \gls{event} $\gamma$ in $P$ if $P(\alpha\mid\beta\cap\gamma)=P(\alpha\mid\gamma)$ or if $P(\beta\cap\gamma)=0$~\cite{koller2009probabilistic}.
\Gls{conditional independence} is denoted with $P\models(\alpha\perp\beta\mid\gamma)$.

\itodo{add example for independence and conditional independence}

Given a \glspl{probability distribution} over a set of \glspl{random variable} $\mathcal{X}$, the final goal is to answer specific questions about this distribution.
These are formulated as queries which are then run against the \glspl{probability distribution}.
One type of queries are called \glspl{probability query}.
A \gls{probability query} consists of two disjoint subsets of \glspl{random variable} of $\mathcal{X}$~\citep{koller2009probabilistic}.
One is called the \gls{evidence}, denoted $\bm{E}$, with its \glspl{random variable} having the instantiation $\bm{e}$.
The second subset contains the query variable and is denoted with $\bm{Y}$.
Based on this, the query is formulated as $P(\bm{Y}|\bm{E}=\bm{e})$~\citep{koller2009probabilistic}.
We thereby want to compute the posterior probability distribution over the assignments $\bm{y}$ to $\bm{Y}$, given the conditioning $\bm{E}=\bm{e}$~\citep{koller2009probabilistic}.

\subsection{Graph Theory}\label{subsec:graph-theory}

%TODO definition graphs, nodes, edges
This subsection will give brief overview of concepts from graph theory that are needed for describing \glspl{crf} and graphical models in general.

\bigskip

% TODO add explanation for converting directed graph to undirected by ignoring the directions of the edges?
A \gls{graph} $\mathcal{K}$ consists of a set of \glspl{node} (also called vertices) $\mathcal{V}$ and a set of \glspl{edge} $\mathcal{E}$.
In a directed \gls{graph}, a pair of \glspl{node} $(v_i,v_j)$ can be connected by directed \glspl{edge} $v_i\to v_j$.
We write $v_i\rightleftharpoons v_j$ to denote that there is some directed edge between $v_i$ and $v_j$.
The \glspl{node} in an undirected graph are connected by undirected \glspl{node} $v_i\undedge v_j$.
In the following, directed \glspl{graph} are denoted with $\mathcal{G}$ and undirected \glspl{graph} with $\mathcal{H}$.
% TODO about child and parent?

Given a \gls{graph} $\mathcal{K} = (\mathcal{V},\mathcal{E})$ and $\mathcal{S}\in\mathcal{V}$, an induced \gls{subgraph} $\mathcal{K}[\mathcal{S}]$ is the \gls{graph} consisting of $(\mathcal{S},\mathcal{E'})$ where $\mathcal{E'}$ is the set of all \glspl{edge} between \glspl{node} in $\mathcal{S}$.
In a complete \gls{subgraph}, every two \glspl{node} in $\mathcal{S}$ are connected by an \gls{edge}.
% TODO add definition of clique?

\subsection{Probabilistic Graphical Models}\label{subsec:graphical-models}
When encoding practical problems with \glspl{probability distribution}, an observation is that ``variables tend to interact only with a very few others''~\citep{koller2009probabilistic}.
This makes it possible to represent such distributions as graphs in a tractable and transparent way, allowing domain experts to evaluate their properties~\citep{koller2009probabilistic}.
\Glspl{probabilistic graphical model} are such representations.
%TODO edge part more precise

\bigskip

In a \gls{probabilistic graphical model}, the set of \glspl{random variable} $\mathcal{X}=\{X_1,\dots,X_n\}$ of a distribution are modeled as \glspl{node} and \glspl{edge} denote a ``direct probabilistic interaction''~\citep{koller2009probabilistic} between the two neighboring nodes.
There are two fundamental groups of graphical models, based on the type of edge that are used:

\bigskip

\Glspl{bayesian network}, usually denoted with $\mathcal{G}$, are encoded with directed \glspl{edge} to build a directed acyclic graph~\citep{koller2009probabilistic}.
An \gls{edge} $X_i\to X_j$ thereby models a direct influence of $X_i$ on $X_j$.
Referring to the example from \Cref{subsec:probability-theory}, $X\to Y$ denotes the influence of the first word in a sequence ending with a period on the second word being capitalized.
Using the chain rule from \todo{add chain rule} it is possible to separate the \gls{joint distribution} $P(\mathcal{X})$ into a set of \glspl{prior distribution} and \glspl{cpd} by considering the direct influences between the \glspl{node}.
See \todo{add} for such a separation of the \gls{joint distribution} $P(X,Y)$ of our example.

\bigskip

\Glspl{markov network}, usually denoted with $\mathcal{H}$, use undirected \glspl{edge} to model a symmetrical influence between two \glspl{random variable}.
Because of this it is not possible to separate the \gls{joint distribution} $P(\mathcal{X})$ into \glspl{prior distribution} and \glspl{cpd}.
Instead, $P(\mathcal{X})$ is represented as the product of \glspl{factor}.
Given a set of \glspl{random variable} $\bm{D}$, a \gls{factor} $\phi$ is a function from $Val(\bm{D})$ to $\realnumbers$~\citep{koller2009probabilistic}.
It is called nonnegative if all its entries are nonnegative and in the following we will consider all our factors to be nonnegative.
$\bm{D}$ is called the \glslink{factor scope}{scope} of $\phi$, denoted by $Scope[\phi]$.
For an example of a factor see \todo{add}.

An important operation on factors is the \gls{factor product}.
Given three disjoint sets of random variables $\bm{X}$, $\bm{Y}$, and $\bm{Z}$, a \gls{factor product} $\phi_1\times \phi_2$ of the two factors $\phi_1(\bm{X},\bm{Y})$ and $\phi_2(\bm{Y},\bm{Z})$ is a factor $\psi(\bm{X},\bm{Y},\bm{Z})$~\citep{koller2009probabilistic}.
The intuition is that the two factors are multiplied by aligning their common set of \glspl{random variable} $\bm{Y}$\todo{example}.

Based on the definition of a \gls{factor product}, we can define an undirected parameterization of a \gls{probability distribution}, called \gls{gibbs distribution}.
A \gls{probability distribution} $P_{\Phi}$ is a Gibbs distribution parameterized by a set of \glspl{factor} $\Phi=\{\phi_1(\bm{D}_1),\dots,\phi_K(\bm{D}_K)\}$ if it is defined as follows:
\begin{equation*}
  P_{\Phi}(X_1,\dots,X_n)=\frac{1}{Z}\tilde{P}_{\Phi}(X_1,\dots,X_n),
\end{equation*}
where
\begin{equation*}
  \tilde{P}_{\Phi}(X_1,\dots,X_n)=\prod_{i=1}^{m}\phi_i(\bm{D}_i)
\end{equation*}
is an unnormalized measure and
\begin{equation*}
  Z=\sum_{X_1,\ldots,X_n}\tilde{P}_{\Phi}(X_1,\dots,X_n)
\end{equation*}
is a normalizing constant called the \gls{partitioning function}~\citep{koller2009probabilistic}.
Having a \gls{gibbs distribution} $P_\Phi$ where each $\bm{D}_k(k=1,\dots,K)$ in $\Phi$ is a complete subgraph of a \gls{markov network} $\mathcal{H}$, we say that $P_\Phi$ factorizes over $\mathcal{H}$~\citep{koller2009probabilistic}\todo{example?}.

One could think that \glspl{factor} are assigned to the edges of a \gls{markov network} in order to parameterize it.
Yet, such an assignment is only able to capture the pairwise interactions between the two incident nodes~\citep{koller2009probabilistic}.
In order to model more complex interactions involving multiple nodes, the \glspl{factor scope} needs to allow an arbitrary subset of nodes in $H$.

Since a factor can be assigned to an arbitrary number of nodes, visualizing \glspl{markov network} by only displaying the random variables as nodes and the independence relations as edges is not sufficient.
Instead, a \gls{factor graph} can be used.
A \gls{factor graph} $\mathcal{F}$ contains two types of nodes: Oval nodes denote variable nodes and squared nodes denote factor nodes~\citep{koller2009probabilistic}.
Each factor node is associated with exactly one factor $\phi$ and each variable node is associated with exactly one random variable.
The graph only contains undirected edges between factor nodes and variable nodes and the scope of $\phi$ is the set of variables that are adjacent to its corresponding factor node~\citep{koller2009probabilistic}\todo{example}.

As we have discussed before, \glspl{factor} are encoded with tables that contain a nonnegative value for each possible assignments in its \glslink{factor scope}{scope}.
Having a set of \glspl{factor} that parameterize a graph. A probability distribution
Another way to parameterize \glspl{factor} is by converting them into log-space~\citep{koller2009probabilistic}.
We can rewrite a factor $\phi(\bm{D})$ as
\begin{equation*}
  \phi(\bm{D}) = \exp(-\epsilon(\bm{D}))
\end{equation*}
where $\epsilon(\bm{D})=-\ln\phi(\bm{D})$ is called \gls{energy function}~\citep{koller2009probabilistic}.

Since we are in the log-space, we can calculate a \gls{probability distribution} over a set of \glspl{random variable} the following way~\citep{koller2009probabilistic}:
\begin{equation*}
  P(X_1,\dots,X_n) \propto \exp\left [-\sum_{i=1}^m\epsilon_i(\bm{D}_i)\right ]
\end{equation*}
Compare this with the \gls{factor product} in the definition of a \gls{gibbs distribution}.

Since we only consider nonnegative \glspl{factor} and the results of the \gls{energy function} can be negative, we need a new definition of a \gls{factor} without its negativity property.
Thereby, given a set of \glspl{random variable} $\bm{D}$, we define a \gls{feature} $f(\bm{D})$ as a function from $\bm{D}$ to $\realnumbers$~\citep{koller2009probabilistic}.
In \todo{example} we apply the \gls{energy function} to the values of \todo{example}.
Values that were set to one in \todo{ref} become zero. \dots\todo{differences}
Consequently following this separation we can define a \gls{probability distribution} $P$ over $\mathcal{H}$ with~\citep{koller2009probabilistic}:
\begin{equation*}
  P(X_1,\dots,X_n) = \frac{1}{Z}\exp\left [-\sum_{i=1}^k w_i f_i(\bm{D}_i)\right ]
\end{equation*}
where $w_1,\dots,w_k$ are weights and $\{f_1(\bm{D}_1),\dots,f_k(\bm{D}_k)\}$ are features with each $\bm{D}_i$ being a complete subgraph in $\mathcal{H}$.
This \gls{probability distribution} is called a \gls{log-linear model}.

\bigskip

After discussing some of the fundamental concepts we can now define \glspl{crf}, a type of \glspl{markov network}. In the following we will address how to encode, inference, and learn them.

\section{Encoding of \glsentryshortpl{crf}}\label{sec:definition-crfs}
A specific kind of \glspl{probabilistic graphical model} are \acrfullpl{crf}.
They were first defined by \citet{lafferty2001conditional} \dots

\bigskip

\Glspl{crf} encode the \gls{conditional probability distribution} $P(\bm{Y}\mid\bm{X})$ where $\bm{Y}$ is a set of \glspl{target variable} and $\bm{X}$ is a set of \glspl{observed variable} with $\bm{X}\cap\bm{Y}=\emptyset$.
A \gls{crf} is constructed using a \gls{markov network} $\mathcal{H}$ where the nodes correspond to $\bm{Y}\cup\bm{X}$ and the undirected edges model a symmetrical influence between the nodes~\citep{koller2009probabilistic}.
Given a set of \glspl{factor} $\Phi=\{\phi_1,\dots\phi_m\}$ that factorize over $\mathcal{H}$, a \gls{crf} defines $P(\bm{Y}\mid\bm{X})$ as~\citep{koller2009probabilistic}:
\begin{equation}
  \label{equ:crf}
  \begin{split}
    P(\bm{Y}\mid\bm{X}) & = \frac{1}{Z(\bm{X})}\tilde{P}(\bm{Y},\bm{X}) \\
    \tilde{P}(\bm{Y},\bm{X}) &= \prod_{i=1}^{m}\phi_i(\bm{D}_i) \\
    Z(\bm{X}) & = \sum_{\bm{Y}}\tilde{P}(\bm{Y},\bm{X})
  \end{split}
\end{equation}

Here, $Z(\bm{X})$ is a normalizing constant, also referred to as partitioning function~\citep{koller2009probabilistic}.
It guarantees that the distribution $P(\bm{Y}\mid\bm{X})$ sums to 1.

\bigskip

A more concrete example for a \gls{crf} can be derived from our author example\dots

\section{Inference of \glsentryshortpl{crf}}\label{sec:inference-crfs}

After discussing the encoding of \glspl{crf} we can now discuss their inference.
More precisely we will consider the inferencing task for \glspl{conditional probability query} (see \Cref{subsec:probability-theory}).

%TODO few sentences about complexity


%TODO exact inference (variable elimination/conditioning?) vs approximate inference (message-passing/optimization problem/particle-based methods?)

\section{Learning of \glsentryshortpl{crf}}\label{sec:learning-crfs}

%TODO see chapter 20 in PGM book
%TODO Limited Memory BFGS (also used in Mallet) (``algorithm of choice'' by andrew2007scalable)

