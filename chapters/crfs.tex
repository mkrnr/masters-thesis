\chapter{Conditional Random Fields}\label{cha:crfs}

As we discussed in the previous chapter, \glspl{crf} are widely used in the related work for learning probabilistic models on given data.
In this chapter we give an introduction to this framework.
First we provide a background in probability theory and graphical models.
In addition to relevant definitions we will use a simplified example (see \todo{ref to Appendix}) that is based on the extraction of author information from reference strings in research papers.
Following this we will introduce the concept of \glspl{crf} and discuss the inference and learning of \gls{crf} models.
In the last section of this chapter we will discuss the application of \gls{crf} on entity recognition on which we will further expand in \todo{add ref}.\\


\section{Probability Theory}\label{sec:probability-theory}
Several concepts from probability theory are crucial for an understanding of \glspl{crf}.
The notion of \glspl{probability distribution} is one such concept.
A \gls{probability distribution} $P$ is defined over $(\glssymbol{outcome space},\glssymbol{measurable set})$ where \glssymbol{outcome space} is the \gls{outcome space} and \glssymbol{measurable set} is a \gls{measurable set} of \glspl{event} \citep{koller2009probabilistic}.
$P$ describes a mapping from events in \glssymbol{measurable set} to real values according to the following rules \citep{koller2009probabilistic}:
\begin{itemize}
  \item $P(\alpha)\geq 0 $ for all $ \alpha \in S$.
  \item $P(\glssymbol{outcome space})=1$.
  \item If $\alpha,\beta\in \glssymbol{outcome space}$ and $\alpha\cap\beta = \emptyset$, then $P(\alpha\cup\beta)=P(\alpha)+P(\beta)$.
\end{itemize}

In our example of author information extraction we define \glssymbol{outcome space} as the set of possible word sequences that can appear in a given text line of a research paper.
For the purpose of illustration we consider four events:

Event $firstPeriod$ contains all word sequences in which the first word of the sequence ends with a period and event $firstNotPeriod$ contains $\Omega\ -\ firstPeriod$.
Event $secondCapitalized$ contains all word sequences in which the second word of the sequence is capitalized and event $secondNotCapitalized$ is defined accordingly.

\glssymbol{measurable set} contains the four defined events and per definition also $\emptyset$ and $\Omega$.
$P$ now assigns all events in \glssymbol{measurable set} a probability.
Given the values in \todo{add ref}, $P(firstPeriod)$ is $XXX$ and $P(secondNotCapitalized)$ is $XXX$.\\

Based on the idea of \glspl{probability distribution}, a \gls{random variable} is a \gls{function} that associates with each outcome in \glssymbol{outcome space} a value \cite{koller2009probabilistic}.
A set of \glspl{random variable} is denoted with bold capital letters and assignments of values to the \glspl{random variable} in such sets are denoted with bold lowercase letters \cite{koller2009probabilistic}.
In addition, $Val(X)$ is the set of values that $X$ can take.
Given a random variable $X$, $P(X)$ is the distribution over events that are described using $X$ \cite{koller2009probabilistic}.
It is referred to as the \gls{marginal distribution} over $X$.
Given a set of \glspl{random variable} $\bm{X}$, $P(\bm{X})$ is called \gls{joint distribution} and assigns each \gls{full assignment} to $\bm{X}$ a probability \cite{koller2009probabilistic}.
A \gls{full assignment} $\xi$ assigns to every variable in $\bm{X}$ a value and thereby $\xi\in Val(\bm{X})$.
Any \gls{event} that is described using $\bm{X}$ must be a union of events that correspond to \glspl{full assignment} to $\bm{X}$.
This gives us a \gls{canonical outcome space} where each outcome is a joint assignment to the \glspl{random variable} in $\bm{X}$.

Following our example we define a \gls{random variable} $Y$ as the function that associates with the first word in a sequence whether or not it end with a period.
Thereby, the \gls{event} $firstPeriod$ can now also be denoted with $Y = period$ and $firstNotPeriod$ with $Y = notPeriod$ resulting in $Val(Y)=\{period, notPeriod\}$.
Consequently, $P(firstPeriod)=P(Y=period)$ and $P(firstNotPeriod)=P(Y=period)$.
$X$ is defined as the function that associates with the second word in a sequence whether or not it is capitalized with $Val(X)=\{capitalized, notCapitalized\}$.
Given $X$ and $Y$, we can build the \gls{joint distribution} $P(X,Y)$.
Using the values from \todo{ref}, $P(X=capitalized,Y=period)=XXX$ and $P(X=capitalized,Y=notPeriod)=XXX$.\\

The \gls{conditional probability} of an event $\alpha$ given $\beta$ with $P(\beta)>0$ is defined as \cite{koller2009probabilistic}:

\begin{equation}
P(\alpha|\beta) = \frac{P(\alpha\cap\beta)}{P(\beta)}
\label{equ:conditional-probability}
\end{equation}

This definition can be extended to \glspl{random variable}.
Given that all assignments in $Y$ are non-zero, the \gls{conditional probability distribution} $P(X|Y)$ assigns each value $Y$ a probability over values of $X$ using the \gls{conditional probability}.
This can also be extended to sets of \glspl{random variable}.

With our example values from \todo{ref}, $P(X=capitalized\ |\ Y=period)=xxx$ and $P(X=capitalized\ |\ Y=notPeriod)=xxx$.

% independence
% conditional independence

% author example

% probability query


\section{Graphical Models}\label{sec:graphical-models}

% observed variable?
% target variable?
