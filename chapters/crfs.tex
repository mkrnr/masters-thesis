\chapter{Conditional Random Fields (\glsentryshortpl{crf})}\label{cha:crfs}

As we discussed in the previous chapter, \glspl{crf} are widely used in the related work for learning probabilistic models on given data.
In this chapter we give an introduction to this framework.
First we provide an overview of relevant concepts in probability theory, graph theory, and graphical models.
In addition to relevant definitions we will use a simplified example (see \todo{ref to Appendix}) that is based on the extraction of author information from reference strings in research papers.
Following this we will introduce the concept of \glspl{crf} and discuss the inference and learning of \gls{crf} models for the task of entity recognition.

\section{Foundations}\label{sec:foundations}
\subsection{Probability Theory}\label{subsec:probability-theory}
Several concepts from probability theory are crucial for an understanding of \glspl{crf} and they all build on the notion of \glspl{probability distribution}.

A \gls{probability distribution} $P$ is defined over $(\glssymbol{outcome space},\glssymbol{measurable set})$ where \glssymbol{outcome space} is the \gls{outcome space} and \glssymbol{measurable set} is a \gls{measurable set} of \glspl{event} \citep{koller2009probabilistic}.
$P$ describes a mapping from events in \glssymbol{measurable set} to real values according to the following rules \citep{koller2009probabilistic}:
\begin{itemize}
  \item $P(\alpha)\geq 0 $ for all $ \alpha \in S$.
  \item $P(\glssymbol{outcome space})=1$.
  \item If $\alpha,\beta\in \glssymbol{outcome space}$ and $\alpha\cap\beta = \emptyset$, then $P(\alpha\cup\beta)=P(\alpha)+P(\beta)$.
\end{itemize}
In our example of author information extraction we define \glssymbol{outcome space} as the set of possible word sequences that can appear in a given text line of a research paper.
For the purpose of illustration we consider four events:

%TODO replace "`firstPeriod"'
Event $firstPeriod$ contains all word sequences in which the first word of the sequence ends with a period and event $firstNotPeriod$ contains $\Omega\ -\ firstPeriod$.
Event $secondCapitalized$ contains all word sequences in which the second word of the sequence is capitalized and event $secondNotCapitalized$ is defined accordingly.

\glssymbol{measurable set} contains the four defined events and per definition also $\emptyset$ and $\Omega$.
$P$ now assigns all events in \glssymbol{measurable set} a probability.
Given the values in \todo{add ref}, $P(firstPeriod)$ is $XXX$ and $P(secondNotCapitalized)$ is $XXX$.

\bigskip

Based on the idea of \glspl{probability distribution}, a \gls{random variable} is a \gls{function} that associates with each outcome in \glssymbol{outcome space} a value~\cite{koller2009probabilistic}.
A set of \glspl{random variable} is denoted with bold capital letters and assignments of values to the \glspl{random variable} in such sets are denoted with bold lowercase letters~\cite{koller2009probabilistic}.
In addition, $Val(X)$ is the set of values that $X$ can take.
Given a \gls{random variable} $X$, $P(X)$ is the \gls{probability distribution}{distribution} over \glspl{event} that are described using $X$~\cite{koller2009probabilistic}.
It is referred to as the \gls{marginal distribution} over $X$.
Given a set of \glspl{random variable} $\bm{X}$, $P(\bm{X})$ is called \gls{joint distribution} and assigns each \gls{full assignment} to $\bm{X}$ a probability~\cite{koller2009probabilistic}.
A \gls{full assignment} $\xi$ assigns to every \gls{random variable} in $\bm{X}$ a value and thereby $\xi\in Val(\bm{X})$.
Any \gls{event} that is described using $\bm{X}$ must be a union of events that correspond to \glspl{full assignment} to $\bm{X}$.
This gives us a \gls{canonical outcome space} where each outcome is a joint assignment to the \glspl{random variable} in $\bm{X}$.

Following our example we define a \gls{random variable} $Y$ as the \gls{function} that associates with the first word in a sequence whether or not it end with a period.
Thereby, the \gls{event} $firstPeriod$ can now also be denoted with $Y = period$ and $firstNotPeriod$ with $Y = notPeriod$ resulting in $Val(Y)=\{period, notPeriod\}$.
Consequently, $P(firstPeriod)=P(Y=period)$ and $P(firstNotPeriod)=P(Y=period)$.
$X$ is defined as the function that associates with the second word in a sequence whether or not it is capitalized with $Val(X)=\{capitalized, notCapitalized\}$.
Given $X$ and $Y$, we can build the \gls{joint distribution} $P(X,Y)$.
Using the values from \todo{ref}, $P(X=capitalized,Y=period)=XXX$ and $P(X=capitalized,Y=notPeriod)=XXX$.

\bigskip

The \gls{conditional probability} of an event $\alpha$ given $\beta$ with $P(\beta)>0$ is defined as~\cite{koller2009probabilistic}:

\begin{equation}
\label{equ:conditional-probability}
P(\alpha\mid\beta) = \frac{P(\alpha\cap\beta)}{P(\beta)}
\end{equation}
This definition can be extended to \glspl{random variable}.
Given that all assignments in $Y$ are non-zero, the \gls{conditional probability distribution} $P(X\mid Y)$ assigns each value $Y$ a probability over values of $X$ using the \gls{conditional probability}.
This can also be extended to sets of \glspl{random variable}.

With our example values from \todo{ref}, $P(X=capitalized\ \mid\ Y=period)=xxx$ and $P(X=capitalized\mid Y=notPeriod)=xxx$.

\bigskip

Another relevant concept is the notion of \gls{independence}.
An \gls{event} $\alpha$ is \glslink{independence}{independent} of \gls{event} $\beta$ in $P$ if $P(\alpha\mid\beta)=P(\alpha)$ or if $P(\beta)=0$~\cite{koller2009probabilistic}.
The \gls{independence} of $\alpha$ and $\beta$ is denoted with $P\models(\alpha\perp\beta)$.
Two \glspl{event} can also be \glslink{independence}{independent} given a third \gls{event}, called \gls{conditional independence}:
$\alpha$ is \glslink{conditional independence}{conditionally independent} of \gls{event} $\beta$ given \gls{event} $\gamma$ in $P$ if $P(\alpha\mid\beta\cap\gamma)=P(\alpha\mid\gamma)$ or if $P(\beta\cap\gamma)=0$~\cite{koller2009probabilistic}.
\Gls{conditional independence} is denoted with $P\models(\alpha\perp\beta\mid\gamma)$.

\itodo{add example for independence and conditional independence}

Given a \glspl{probability distribution} over a set of \glspl{random variable} $\mathcal{X}$, the final goal is to answer specific questions about this distribution.
These are formulated as queries which are then run against the \glspl{probability distribution}{distribution}.
One type of queries are called \glspl{probability query}.
A \gls{probability query} consists of two disjoint subsets of \glspl{random variable} of $\mathcal{X}$~\citep{koller2009probabilistic}.
One is called the \gls{evidence}, denoted $\bm{E}$, with its \glspl{random variable} having the assignments $\bm{e}$.
%TODO better explain assignments (earlier)
The second subset contains the query variables and is denoted with $\bm{Y}$.
Based on this, the query is formulated as $P(\bm{Y}|\bm{E}=\bm{e})$~\citep{koller2009probabilistic}.
We thereby want to compute the posterior \gls{probability distribution} over the assignments $\bm{y}$ to $\bm{Y}$, given the conditioning $\bm{E}=\bm{e}$~\citep{koller2009probabilistic}.

\subsection{Graph Theory}\label{subsec:graph-theory}

This subsection will give brief overview of concepts from graph theory that are needed for describing \glspl{crf} and graphical models in general.

\bigskip

% TODO add explanation for converting directed graph to undirected by ignoring the directions of the edges?
A \gls{graph} $\mathcal{K}$ consists of a set of \glspl{node} (also called vertices) $\mathcal{V}$ and a set of \glspl{edge} $\mathcal{E}$.
In a directed \gls{graph}, a pair of \glspl{node} $(v_i,v_j)$ can be connected by directed \glspl{edge} $v_i\to v_j$.
We write $v_i\rightleftharpoons v_j$ to denote that there is some directed edge between $v_i$ and $v_j$.
The \glspl{node} in an undirected graph are connected by undirected \glspl{node} $v_i\undedge v_j$.
In the following, directed \glspl{graph} are denoted with $\mathcal{G}$ and undirected \glspl{graph} with $\mathcal{H}$.
% TODO about child and parent?

Given a \gls{graph} $\mathcal{K} = (\mathcal{V},\mathcal{E})$ and $\mathcal{S}\in\mathcal{V}$, an induced \gls{subgraph} $\mathcal{K}[\mathcal{S}]$ is the \gls{graph} consisting of $(\mathcal{S},\mathcal{E'})$ where $\mathcal{E'}$ is the set of all \glspl{edge} between \glspl{node} in $\mathcal{S}$.
In a complete \gls{subgraph}, every two \glspl{node} in $\mathcal{S}$ are connected by an \gls{edge}.
% TODO add definition of clique?

\subsection{Probabilistic Graphical Models}\label{subsec:graphical-models}
When encoding practical problems with \glspl{probability distribution}, a key insight is that ``variables tend to interact only with a very few others''~\citep{koller2009probabilistic}.
This makes it possible to represent such distributions as graphs in a tractable and transparent way, allowing domain experts to evaluate their properties~\citep{koller2009probabilistic}.
\Glspl{probabilistic graphical model} are such representations.

\bigskip

In a \gls{probabilistic graphical model}, the \glspl{random variable} $\mathcal{X}$ of a distribution are modeled as \glspl{node} where each \gls{node} represents one \gls{random variable} in $\mathcal{X}$. An \glspl{edge} then denotes a ``direct probabilistic interaction''~\citep{koller2009probabilistic} between its two two incident \glspl{node}.

% TODO edge part more precise
There are two fundamental groups of graphical models, based on the type of edge that are used: \glspl{bayesian network} and \glspl{markov network}.

\bigskip

\Glspl{bayesian network}, usually denoted with $\mathcal{G}$, are encoded with directed \glspl{edge} to build a directed acyclic graph~\citep{koller2009probabilistic}.
An \gls{edge} $X_i\to X_j$ thereby models a direct influence of $X_i$ on $X_j$.
Referring to the example from \Cref{subsec:probability-theory}, $X\to Y$ denotes the influence of the first word in a sequence ending with a period on the second word being capitalized.
Using the chain rule from \todo{add chain rule} it is possible to separate the \gls{joint distribution} $P(\mathcal{X})$ into a set of \glspl{prior distribution} and \glspl{cpd} by considering the direct influences between the \glspl{node}.
See \todo{add} for such a separation of the \gls{joint distribution} $P(X,Y)$ of our example.

\bigskip

\Glspl{markov network}, usually denoted with $\mathcal{H}$, use undirected \glspl{edge} to model a symmetrical influence between two \glspl{random variable}.
Because of this it is not possible to separate the \gls{joint distribution} $P(\mathcal{X})$ into \glspl{prior distribution} and \glspl{cpd}.
Instead, $P(\mathcal{X})$ is represented as the product of \glspl{factor}.
Given a set of \glspl{random variable} $\bm{D}$, a \gls{factor} $\Psi$ is a function from $Val(\bm{D})$ to $\realnumbers$~\citep{koller2009probabilistic}.
It is called nonnegative if all its entries are nonnegative and in the following we will consider all our factors to be nonnegative.
$\bm{D}$ is called the \glslink{factor scope}{scope} of $\Psi$, denoted by $Scope[\Psi]$.
For an example of a factor see \todo{add}.

An important operation on factors is the \gls{factor product}.
Given three disjoint sets of random variables $\bm{X}$, $\bm{Y}$, and $\bm{Z}$, a \gls{factor product} $\Psi_1\times \Psi_2$ of the two factors $\Psi_1(\bm{X},\bm{Y})$ and $\Psi_2(\bm{Y},\bm{Z})$ is a factor $\Psi(\bm{X},\bm{Y},\bm{Z})$~\citep{koller2009probabilistic}.
The intuition is that the two factors are multiplied by aligning their common set of \glspl{random variable} $\bm{Y}$\todo{example}.

Based on the definition of a \gls{factor product}, we can define an undirected parameterization of a \gls{probability distribution}, called \gls{gibbs distribution}.
A \gls{probability distribution} $P(X_1,\dots,X_N)$ is a \gls{gibbs distribution} parameterized by a set of \glspl{factor} $\{\Psi_1(\bm{D}_1),\dots,\Psi_K(\bm{D}_K)\}$ if it is defined as~\citep{koller2009probabilistic}
\begin{equation}
  \label{equ:gibbs-distribution}
  \begin{split}
  P(X_1,\dots,X_N) & =\frac{1}{Z}\tilde{P}(X_1,\dots,X_N) \\
  \tilde{P}(X_1,\dots,X_N) & =\prod_{k=1}^{K}\Psi_k(\bm{D}_k) \\
  Z & =\sum_{X_1,\ldots,X_N}\tilde{P}(X_1,\dots,X_N)
  \end{split}
\end{equation}
where is a $\tilde{P}(X_1,\dots,X_N)$ is an unnormalized measure and $Z$ is a normalizing constant, sometimes called the \gls{partitioning function}, which guarantees that the \gls{probability distribution} sums to 1.
Having a \gls{gibbs distribution} $P$ where each $\bm{D}_k(k=1,\dots,K)$ in  is a complete subgraph of a \gls{markov network} $\mathcal{H}$, we say that $P$ factorizes over $\mathcal{H}$~\citep{koller2009probabilistic}\todo{example?}.

One could think that \glspl{factor} are assigned to the edges of a \gls{markov network} in order to parameterize it.
Yet, such an assignment is only able to capture the pairwise interactions between the two incident nodes~\citep{koller2009probabilistic}.
In order to model more complex interactions involving multiple nodes, the \glspl{factor scope} needs to allow an arbitrary subset of nodes in $H$.

Since a factor can be assigned to an arbitrary number of nodes, visualizing \glspl{markov network} by only displaying the random variables as nodes and the independence relations as edges is not sufficient.
Instead, a \gls{factor graph} can be used.
A \gls{factor graph} $\mathcal{F}$ contains two types of nodes: Oval nodes denote variable nodes and squared nodes denote factor nodes~\citep{koller2009probabilistic}.
Each factor node is associated with exactly one factor $\Psi$ and each variable node is associated with exactly one random variable.
The graph only contains undirected edges between factor nodes and variable nodes and the scope of $\Psi$ is the set of variables that are adjacent to its corresponding factor node~\citep{koller2009probabilistic}\todo{example}.

As we have discussed before, \glspl{factor} are encoded with tables that contain a nonnegative value for each possible assignments in its \glslink{factor scope}{scope}.
Having a set of \glspl{factor} that parameterize a graph. A probability distribution
Another way to parameterize \glspl{factor} is by converting them into log-space~\citep{koller2009probabilistic}.
We can rewrite a factor $\Psi(\bm{D})$ as
\begin{equation*}
  \label{equ:energy-function}
  \Psi(\bm{D}) = \exp(-\epsilon(\bm{D}))
\end{equation*}
where $\epsilon(\bm{D})=-\ln\Psi(\bm{D})$ is called \gls{energy function}~\citep{koller2009probabilistic}.

Since we are in the log-space, we can calculate a \gls{probability distribution} over a set of \glspl{random variable} the following way~\citep{koller2009probabilistic}:
\begin{equation*}
  \label{equ:p-energy-function}
  P(X_1,\dots,X_N) \propto \exp\left [-\sum_{k=1}^K\epsilon_k(\bm{D}_k)\right ]
\end{equation*}
Since we only consider nonnegative \glspl{factor} and the results of the \gls{energy function} can be negative, we need a new definition of a \gls{factor} without its negativity property.
Thereby, given a set of \glspl{random variable} $\bm{D}$, we define a \gls{feature function} $f(\bm{D})$ as a function from $\bm{D}$ to $\realnumbers$~\citep{koller2009probabilistic}.
In \todo{example} we apply the \gls{energy function} to the values of \todo{example}.
Values that were set to one in \todo{ref} become zero. \dots\todo{differences}
Consequently following this separation we can define a \gls{probability distribution} $P$ over $\mathcal{H}$ with~\citep{koller2009probabilistic}:
\begin{equation}
  \label{equ:log-linear-model}
  P(X_1,\dots,X_N) = \frac{1}{Z}\exp\left [-\sum_{k=1}^K \theta_k f_k(\bm{D}_k)\right ]
\end{equation}
where $\theta_1,\dots,\theta_K$ are weights and $\{f_1(\bm{D}_1),\dots,f_K(\bm{D}_K)\}$ are \glspl{feature function} with each $\bm{D}_k$ being a complete subgraph in $\mathcal{H}$.
This \gls{probability distribution} is called a \gls{log-linear model}.

\bigskip

In addition to \glspl{bayesian network} and \glspl{markov network} we can further distinguish between \glspl{generative model} and \glspl{discriminative model}.
Assume a set of input variables, also called \glspl{observed variable}, $\bm{X}$ and a set of output variables, also called \glspl{target variable} $\bm{Y}$ with $\bm{X}\cap\bm{Y}=\emptyset$.
A \gls{generative model} then encodes the \gls{joint distribution} $P(\bm{Y},\bm{X})$ whereas a \gls{discriminative model} encodes the \gls{conditional probability distribution} $P(\bm{Y}\mid\bm{X})$~\citep{koller2009probabilistic}.
More precisely, for a \gls{generative model} we have $P(\bm{Y},\bm{X})=P(\bm{Y})P(\bm{X}\mid\bm{Y})$.
We thereby consider how the output of the model is generated as a function of the input~\citep{sutton2010introduction}.

This leads to the main difference between the two models, namely that for a \gls{discriminative model} we do not need to model $P(\bm{X})$.
\citet{sutton2010introduction} argue that indeed the modeling of $P(\bm{X})$ in \glspl{generative model} leads to a number of difficulties and limitations.
According to them, $P(\bm{X})$ often contains a number of highly dependent features which restrict the modeling.
For example, in \gls{nlp} tasks we often model word-identities as features.
Having a limited training set, we often have words that were unseen during training.
In order to still give a reasonable classification for unseen words it would be beneficial to also include other features in addition to just the word-identities~\citep{sutton2010introduction}.
Such features could be the capitalization, length, and prefixes of a word or even its neighboring words~\citep{sutton2010introduction}.
Yet, such features are highly dependent to each other and such dependencies would need to be represented in a \gls{generative model} which is often intractable in practice~\citep{sutton2010introduction}.
\Glspl{discriminative model} on the other hand can leverage such features despite their high dependencies since $P(\bm{X})$ is not modeled~\citep{koller2009probabilistic}.
%TODO argue about modeling p(Y|X) directly since it represents our probability query (sutton?)

Since in \glspl{generative model} $P(\bm{Y})$ topologically precedes $P(\bm{X}\mid\bm{Y})$, \citet{sutton2010introduction} argue that they are more naturally modeled by a \gls{bayesian network}.
Since there is no such order in \glspl{discriminative model}, it is argued that they are more naturally modeled by a \gls{markov network}~\citep{sutton2010introduction}.

\bigskip

After introducing some of the fundamental concepts we can now discuss \glspl{crf}. In the following we will address how to encode, inference, and learn them.

\section{Encoding of \glsentryshortpl{crf}}\label{sec:definition-crfs}
A popular framework for building \glspl{probabilistic graphical model} are \acrfullpl{crf}.
They were proposed by \citet{lafferty2001conditional} with the goal of segmenting and labeling sequence data.
A main motivation for \glspl{crf} was to overcome a label bias problem that other discriminative \glspl{markov network}, such as \glspl{memm}, tend to have~\citep{lafferty2001conditional}.
\citet{lafferty2001conditional} argue that this is due to the structure in which models such as \glspl{memm} model \glspl{conditional probability} using per-state models.
This can lead to a bias towards stats which have fewer outgoing transitions~\citep{lafferty2001conditional}.
In order to overcome this bias, \glspl{crf} do not have per-state models but instead contain a single model to represent ``the \glslink{joint distribution}{joint probability} of the entire sequence of labels given the observation sequence''~\citep{lafferty2001conditional}.

\bigskip

\Glspl{crf} encode the \gls{conditional probability distribution} $P(\bm{Y}\mid\bm{X})$ where $\bm{Y}$ is a set of \glspl{target variable} and $\bm{X}$ is a set of \glspl{observed variable} with $\bm{Y}\cap\bm{X}=\emptyset$.
A \gls{crf} is constructed using a \gls{markov network} $\mathcal{H}$ where the nodes correspond to $\bm{Y}\cup\bm{X}$ and the undirected edges model a symmetrical influence between the nodes~\citep{koller2009probabilistic}.
Given a set of \glspl{factor} $\{\Psi_1,\dots\Psi_K\}$ that factorize over $\mathcal{H}$, a \gls{crf} defines $P(\bm{Y}\mid\bm{X})$ as~\citep{koller2009probabilistic}
\begin{equation}
  \label{equ:crf-factor}
  \begin{split}
    P(\bm{Y}\mid\bm{X}) & = \frac{1}{Z(\bm{X})}\tilde{P}(\bm{Y},\bm{X}) \\
    \tilde{P}(\bm{Y},\bm{X}) &= \prod_{k=1}^{K}\Psi_k(\bm{D}_k) \\
    Z(\bm{X}) & = \sum_{\bm{Y}}\tilde{P}(\bm{Y},\bm{X})
  \end{split}
\end{equation}
where, similar to \glspl{gibbs distribution} (see \Cref{equ:gibbs-distribution}), $\tilde{P}(\bm{Y},\bm{X})$ is the unnormalized measure and $Z(\bm{X})$ is a normalizing constant~\citep{koller2009probabilistic}.
In fact, the only difference between the definition of a \gls{gibbs distribution} in \Cref{equ:gibbs-distribution} and the definition of a \gls{crf} above is how the normalizing constant $Z$ is defined~\citep{koller2009probabilistic}.
In \Cref{equ:gibbs-distribution}, $Z$ normalizes $\tilde{P}(X_1,\dots,X_N)$ with a sum over all possible assignments to $X_1,\dots,X_N$ resulting in a \gls{joint distribution} $P(X_1,\dots,X_N)$.
In \Cref{equ:crf-factor}, however, $Z(\bm{X})$ normalizes $\tilde{P}(\bm{Y},\bm{X})$ with a sum over the assignments $\bm{y}$ to $\bm{Y}$ over the specified assignments $\bm{x}$ to $\bm{X}$.
This way of normalizing results in the \gls{conditional probability distribution} $P(\bm{Y}\mid\bm{X})$ (Compare with \Cref{equ:conditional-probability}).

\bigskip

Using \Cref{equ:log-linear-model} we can reformulate the definition of \glspl{crf} in \Cref{equ:crf-factor} as a \gls{log-linear model}:
\begin{equation}
  \label{equ:crf-log-linear}
  \begin{split}
    P(\bm{Y}\mid\bm{X}) & = \frac{1}{Z(\bm{X})}\tilde{P}(\bm{Y},\bm{X}) \\
    \tilde{P}(\bm{Y},\bm{X}) & = \exp\left\{ -\sum_{k=1}^K \theta_k f_k(\bm{D}_k)\right\} \\
    Z(\bm{X}) & = \sum_{\bm{Y}}\tilde{P}(\bm{Y},\bm{X})
  \end{split}
\end{equation}
This representation allows us to encode a \gls{crf} model more compactly using \glspl{feature function} instead of \glspl{factor}.
This will become more clear in the following discussion.

\bigskip

A specific kind of \gls{crf} that follows a rather simple structure are \glspl{linear-chain crf}.
For given $\bm{X}=\{X_1,\dots,X_N\}$ and $\bm{Y}=\{Y_1,\dots,Y_N\}$, we consider two types of \glspl{factor}:
\begin{itemize}
  \item $\Psi_n(Y_n,Y_{n-1})$ models the dependency between the \gls{target variable} $Y_n$ and its preceding \gls{target variable} $Y_{n-1}$.
  \item $\Psi_n(Y_n,\bm{\tilde{X}}_n)$ models the dependency between the \gls{target variable} $Y_n$ and its context given by the \glspl{observed variable} $\bm{\tilde{X}}_n=\{\tilde{X}_1,\dots,\tilde{X}_T\}$ with $\bm{\tilde{X}}_n\subseteq\bm{X}$.
    $T$ can be different for every $\bm{\tilde{X}_n}$.
\end{itemize}
Inserting the two factors in \Cref{equ:crf-factor} results in:
\begin{equation}
  \label{equ:linear-chain-crf-factor}
  \begin{split}
    P(\bm{Y}\mid\bm{X}) & = \frac{1}{Z(\bm{X})}\tilde{P}(\bm{Y},\bm{X}) \\
    \tilde{P}(\bm{Y},\bm{X}) &= \prod_{n=1}^{N}\Big(\Psi_n(Y_n,Y_{n-1})\times\Psi_n(Y_n,\bm{\tilde{X}}_n)\Big) \\
    Z(\bm{X}) & = \sum_{\bm{Y}}\tilde{P}(\bm{Y},\bm{X})
  \end{split}
\end{equation}
We now can represent the two \glspl{factor} using the \glspl{feature function}
\begin{equation}
  \label{equ:linear-chain-crf-feature-functions}
  \begin{split}
    \tilde{f}_k(Y_n,Y_{n-1})\equalsdef f_{j,i}(Y_n,Y_{n-1}) & = \bm{\mathbbm{1}}\{Y_n=j,Y_{n-1}=i\}\\
    \tilde{f}_l(Y_n,\tilde{X}_t)\equalsdef f_{j,\tilde{x}_t}(Y_n,\tilde{X}_t) & = \bm{\mathbbm{1}}\{Y_n=j,\tilde{X}_t=\tilde{x}_t\}
  \end{split}
\end{equation}
where $i$, $j$, and $\tilde{x}_t$ are predefined assignments to $Y_{n-1}$, $Y_n$, and $\tilde{X}_t$ respectively. $\bm{\mathbbm{1}}$ is an indicator function that returns $1$ if all listed \glspl{random variable} match their predefined assignments.
The $\tilde{f}$ notation with its indices will later allow us to iterate over the predefined \glspl{feature function}.

In other words, $\tilde{f}_k(Y_n,Y_{n-1})$ models a specific dependency between the neighboring \glspl{target variable} $Y_n$ and $Y_{n-1}$. $\tilde{f}_l(Y_n,\tilde{X}_t)$ models a specific dependency between the \gls{target variable} $Y_n$ and one of the \glspl{observed variable} $\tilde{X}_t$ in context of $Y_n$ given by $\bm{\tilde{X}}_n$.

By inserting the two types of \glspl{feature function} from \Cref{equ:linear-chain-crf-feature-functions} into \Cref{equ:crf-log-linear} we define \glspl{linear-chain crf} as:
\begin{equation}
  \label{equ:linear-chain-crf-log-linear}
  \begin{split}
    P(\bm{Y}\mid\bm{X}) & = \frac{1}{Z(\bm{X})}\tilde{P}(\bm{Y},\bm{X})  \\
    \tilde{P}(\bm{Y},\bm{X}) & = \exp\left\{ -\sum_{n=1}^N \Big(\sum_{k=1}^K\theta_k \tilde{f}_k(Y_n,Y_{n-1})+\sum_{l=1}^L\theta_l \tilde{f}_l(Y_n,\tilde{X}_t)\Big) \right\} \\
    Z(\bm{X}) & = \sum_{\bm{Y}}\tilde{P}(\bm{Y},\bm{X})
  \end{split}
\end{equation}
Thereby, for every $Y_n$, we iterate over $K$ predefined \glspl{feature function} $\tilde{f}_k$ and $L$ predefined \glspl{feature function} $\tilde{f}_l$ with $\tilde{X}_t\in\bm{\tilde{X}}_n$.
In order to simplify the notation, we define $Y_0$ as a special start state, denoted \texttt{start}~\citep{lafferty2001conditional}.

An example for a  \gls{linear-chain crf} can be derived from our author example\dots\todo{example}

Returning to our argument that \glspl{feature function} allow are move compact encoding of \glspl{crf} than \glspl{factor}, compare \todo{example}.

\citet{sutton2010introduction} show that \glspl{hmm} are a restricted kind of \gls{linear-chain crf}, namely one where $\bm{\tilde{X}}_n$ in \Cref{equ:linear-chain-crf-feature-functions} only includes the word's identity at the current position $n$.

\section{Inference of \glsentryshortpl{crf}}\label{sec:inference-crfs}

After formalizing the encoding of \glspl{crf} we can now discuss their inference.
Here we consider the inferencing task for \glspl{probability query} $P(\bm{Y}|\bm{X}=\bm{x})$ (see \Cref{subsec:probability-theory}).
\citet{sutton2010introduction} distinguish two kinds of inference problems:
\begin{enumerate}
\item Given a trained model and observed assignments $\bm{X}=\bm{x}$, predict the most likely assignment $\bm{Y}=\bm{y}$:
    \begin{equation}
      \label{equ:inference-argmax}
      \argmax\limits_{\bm{y}} P(\bm{Y}=\bm{y}\mid\bm{X}=\bm{x})
    \end{equation}
  \item During the parameter estimation during learning, compute the \gls{marginal distribution} for \dots\todo{add based on learning chapter}
\end{enumerate}
%TODO about that 1. and 2. are same fundamental operations (sutton2010, p. 27)
By replacing the summation operator over $\bm{y}$ in the marginalization in the second case by the $\argmax$ function, we effectively result in the prediction problem in the first case.
Thereby, the two kinds of inference problems can be seen as fundamentally the same operation~\citep{sutton2010introduction}.
In the following we will focus on the inference of the normalizing constant $Z(\bm{X})$ in the context of \glspl{linear-chain crf}.
An approach to the prediction task can be derived from it.

\bigskip

In order to show the computational complexity of this inferencing task, we will consider \glspl{linear-chain crf} on a \gls{factor} level.
According to \Cref{equ:linear-chain-crf-factor} we have:
\begin{equation}
  \label{equ:linear-chain-crf-z}
  Z(\bm{X}) = \sum_{\bm{Y}}\bigg(\prod_{n=1}^{N}\Big(\Psi_n(Y_n,Y_{n-1})\times\Psi_n(Y_n,\bm{\tilde{X}}_n)\Big)\bigg)
\end{equation}
Since we calculate the \gls{factor product} for every possible joint assignment $\bm{Y}=\bm{y}$, the computation of $Z(\bm{X})$ is exponential \dots
\itodo{show complexity with example?}

Yet, in the case of \glspl{linear-chain crf}, it is possible to reduce the complexity of calculating $Z(\bm{X})$ by using the forward-backward algorithm, sometimes referred to as dynamic programming or variable elimination~\citep{sutton2010introduction,koller2009probabilistic}.
The key insight of the forward-backward algorithm is that during the na\"{\i}ve calculation of, in our case, $Z(\bm{X})$, partial calculations are repeated exponentially often.
In order to prevent unnecessary repetitions of the same calculations, intermediate results are stored and reused.
The following demonstration is based on the calculation of $P(\bm{X})$ for \glspl{hmm} using the forward-backward algorithm in \citet{sutton2010introduction}.

\bigskip

By applying the distributive law we can reorder \Cref{equ:linear-chain-crf-z} in the following way:
\begin{equation}
  \label{equ:linear-chain-crf-z-distributive}
  \begin{split}
  Z(\bm{X}) = & \sum_{Y_N}\Bigg(\sum_{Y_{N-1}}\Big(\Psi_N(Y_N,Y_{N-1})\times\Psi_N(Y_N,\bm{\tilde{X}}_N)\Big)\bigg(\sum_{Y_{N-2}}\Big(\Psi_{N-1}(Y_{N-1},Y_{N-2})\\
  & \times\Psi_{N-1}(Y_{N-1},\bm{\tilde{X}}_{N-1})\Big) \dots\bigg)\Bigg)
  \end{split}
\end{equation}
As we can see, the inner sums are needed multiple times by the outer sums.
In a na\"{\i}ve approach, these inner sums would be recalculated every time they appear in an outer sum.\todo{example?}
Instead, our goal is to calculate the inner sums once and store their result for later usage.

\bigskip

In order to find a recursive definition of $Z(\bm{X})$ that allows the reuse of intermediate calculations, we first consider the following case
\begin{equation}
  \label{equ:linear-chain-crf-z-j-1}
  \begin{split}
    Z(\bm{X},Y_N=j) = & \sum_{Y_1,\dots Y_{N-1}}\bigg(\Big(\Psi_N(j,Y_{N-1})\times\Psi_N(j,\bm{\tilde{X}}_N)\Big) \\
    & \times\prod_{t'=1}^{N-1}\Big(\Psi_{t'}(Y_{t'},Y_{t'-1})\times\Psi_{t'}(Y_{t'},\bm{\tilde{X}}_{t'})\Big)\bigg)
  \end{split}
\end{equation}
where we have an assignment $Y_N=j$ and where the calculations including $j$ are isolated from the product over all other calculations.

Since in the two \glspl{factor} $\Psi_N$ we only use the assignments to $Y_{N-1}$ from the sum over $Y_1,\dots,Y_{N-1}$, we can rewrite \Cref{equ:linear-chain-crf-z-j-1} as
\begin{equation}
  \label{equ:linear-chain-crf-z-j-2}
  \begin{split}
    Z(\bm{X},Y_N=j) = & \sum_{Y_{N-1}}\bigg(\Big(\Psi_N(j,Y_{N-1})\times\Psi_N(j,\bm{\tilde{X}}_N)\Big) \\
    & \times\sum_{Y_1,\dots,Y_{N-1}}\prod_{t'=1}^{N-1}\Big(\Psi_{t'}(Y_{t'},Y_{t'-1})\times\Psi_{t'}(Y_{t'},\bm{\tilde{X}}_{t'})\Big)\bigg)
  \end{split}
\end{equation}
and using the definition of $Z(\bm{X})$ in \Cref{equ:linear-chain-crf-z} we arrive at
\begin{equation}
  \label{equ:linear-chain-crf-z-j-3}
  Z(\bm{X},Y_N=j) = \sum_{Y_{N-1}}\bigg(\Big(\Psi_N(j,Y_{N-1})\times\Psi_N(j,\bm{\tilde{X}}_N)\Big)\times Z(X_1,\dots,X_{N-1})\bigg).
  \end{equation}
Generalizing from this case we can now define
\begin{equation}
  \label{equ:linear-chain-crf-z-alpha}
  \alpha_n(j) = \sum_{i\in\bm{Y}}\bigg(\Big(\Psi_n(j,i)\times\Psi_n(j,\bm{\tilde{X}}_n)\Big)\times \alpha_{n-1}(i)\bigg)
\end{equation}
where the position of assignment $i$ in $\bm{Y}$ is derived from the definition $\Psi_n(j,i)$.
Thereby, if $Y_n=j$ then $i$ is an assignment to $Y_{n-1}$.
The base case for the recursion is defined as
\begin{equation}
  \label{equ:linear-chain-crf-z-alpha-base}
  \alpha_1(j) = \Psi_1(j,y_0)\times\Psi_1(j,\bm{\tilde{X}}_1)
\end{equation}
where $y_0$ again is the \texttt{start} state. Since the calculation is recursive in a way that $\alpha_n$ is calculated using the result of $\alpha_{n-1}$, we refer to $\alpha_n$ as a \textit{forward variable}~\citep{sutton2010introduction}. In order to calculate $Z(\bm{X})$ using $\alpha_n(j)$, we calculate the sum over all assignments to $Y_T$ in $\bm{Y}$, resulting in
\begin{equation}
  \label{equ:linear-chain-crf-z-alpha-sum}
  Z(\bm{X})=\sum_{Y_T\in\bm{Y}}\alpha_T(Y_T).
\end{equation}

\bigskip

Even though the forward variable $\alpha_n$ is sufficient for calculating $Z(\bm{X})$, other calculations may also require a \textit{backward variable} $\beta_n$. It is defined analogously to $\alpha_n$ but the recursion expands in the opposite direction such that the calculation of $\beta_n$ recursively uses the result of $\beta_{n+1}$:
\begin{equation}
  \label{equ:linear-chain-crf-z-beta}
  \beta_n(i) = \sum_{j\in\bm{Y}}\bigg(\Big(\Psi_{n+1}(j,i)\times\Psi_{n+1}(j,\bm{\tilde{X}}_{n+1})\Big)\times \beta_{n+1}(j)\bigg)
\end{equation}
Due to the opposite order, the base case for the recursion is defined as $\beta_N(i)=1$.
Also, note the inverted usage of $i$ in $\beta_n(i)$ and $j$ in $\beta_{n+1}(j)$ in comparison with the usage in \Cref{equ:linear-chain-crf-z-alpha} for $\alpha_n$.
It is also possible to calculate $Z(\bm{X})$ using $\beta_n$ with
\begin{equation}
  \label{equ:linear-chain-crf-z-beta-sum}
  Z(\bm{X})=\beta_0(Y_0)\equalsdef\sum_{Y_1\in\bm{Y}}\bigg(\Big(\Psi_1(Y_1,Y_0)\times\Psi_1(Y_1,\bm{\tilde{X}}_1)\Big)\times\beta_1(Y_1)\bigg)
\end{equation}
in order to correctly handle the \texttt{start} state $Y_0$.
Again, the definitions of $\alpha_n$ and $\beta_n$ are derived from \citet{sutton2010introduction} and their example for \glspl{hmm}.

\bigskip

By using both $\alpha_n$ and $\beta_n$ it is possible to approach a calculation from both sides of a sequence\todo{example for $\tilde{P}(\bm{Y},\bm{X})$, sutton2010 p.34 top}

%TODO few words on Viterbi algorithm (sutton2010)

%TODO exact inference (variable elimination/conditioning?) vs approximate inference (message-passing/optimization problem/particle-based methods?)
\bigskip

In the following section we will discuss methods for learning \glspl{crf}.
In particular we will look into the parameter estimation of the weights $\theta$ that are assigned to \glspl{feature function}, for example in \Cref{equ:linear-chain-crf-log-linear}.
\section{Learning of \glsentryshortpl{crf}}\label{sec:learning-crfs}


%TODO see chapter 20 in PGM book
%TODO approximation using Limited Memory BFGS (also used in Mallet) (``algorithm of choice'' by andrew2007scalable)

