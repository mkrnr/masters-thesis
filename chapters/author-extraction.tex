\chapter{Author Extraction}\label{cha:author-extraction}

\newcommand\researchquestionformat[1]{\begin{quote}#1\end{quote}}

\newcommand\researchquestionone{\researchquestionformat{%
  \RQ{1}: Given a set of unlabeled reference sections from the research area of German social sciences, does using an author list that is related to this area improve the performance of the learned \gls{crf} model in comparison with using an unrelated author list?
}}

\newcommand\researchquestiontwo{\researchquestionformat{%
  \RQ{2}: Given a set of reference sections from the research area of German social sciences with matched author names, does a labeling using the \gls{bieo} improve the performance of the learned \gls{crf} model in comparison with a labeling using the \gls{bio} format?
}}

\newcommand\researchquestionthree{\researchquestionformat{%
  \RQ{3}: Given a set of reference sections from the research area of German social sciences with matched author names, how does, for a word $w_n$ that has no matched author names, modifying the probability mass distribution over the labels in $Val(Y_n)$ impact the performance of a \gls{crf} model that was learned on the resulting \gls{ge} constraints?
}}

\newcommand\researchquestionfour{\researchquestionformat{%
  \RQ{4}: Given a set of reference sections from the research area of German social sciences with matched author names, how does changing the number of considered words $w_n$ which do not have matched author names impact the performance of a \gls{crf} model that was learned on the resulting \gls{ge} constraints?
}}

In this chapter we will discuss author extraction as a concrete use case for the usage of \glspl{crf} in combination with \gls{distant supervision}.
This can be broken down into to following steps:
\begin{enumerate}
  \item Preprocessing of research papers to extract reference sections as text from \gls{pdf} documents
  \item Generating tagged training sets for distant supervision
  \item Building \gls{ge} constraints
  \item Learning a \gls{crf} model using \gls{ge} constraints
\end{enumerate}
The following sections will describe these steps in more detail.
In addition we will formulate a number of research questions that we aim to answer in \Cref{cha:evaluation}.
These research questions will have a focus on the author extraction from research papers in the area of German social sciences.

\section{Preprocessing}\label{sec:ae-preprocessing}

Before building a training set using \gls{distant supervision}, an unlabeled training data set needs to be generated.

Since we want to learn a model that is able to extract author names in reference sections, it is crucial to remove the text that is not part of the reference section.
Otherwise, we would also match names that appear in the body of the research paper during the author name matching step (see \Cref{subsec:ae-author-name-matching}).
In order to incorporate the layout of the research paper, this step of detecting reference sections ideally should be performed before converting the \gls{pdf} document into a textual format.
This would allow, for example, an accurate detection of headers or page numbers.

\bigskip

Either before or after detecting reference sections, the textual content of the research paper has to be extracted.
For this, the layout of the document needs to be recognized in order to, for example, correctly extract the text from a research paper that contains two columns of text per page.

\section{Generating Training Sets with Distant Supervision}\label{sec:ae-distant-supervision}

As discussed in \todo{ref}, distant supervision is \dots
In this section we discuss our approach of building such a distantly supervised training set for, our use case, author name extraction.


\subsection{Knowledge Base Creation}\label{subsec:ae-knowledge-base-creation}

In order to apply distant supervision to the task of labeling author names, an external source for author names is needed.
Since the goal is to distinguish between the first names and last names of an author, external sources that provide this distinction are preferable.
This is due to the fact that determining which part of a name belongs to the first names and which to the last names is not trivial\todo{example}.
In addition to labeling reference sections for the distant supervision, such a knowledge base can also be used to construct features for the \gls{crf} model (see \Cref{subsec:ae-fearture-engineering}).

\bigskip

A question that arises is whether the origin of the author list is of importance.
We formulate this as the following research question:
\researchquestionone%

\subsection{Author Name Matching}\label{subsec:ae-author-name-matching}

Given a data set of unlabeled reference sections and a knowledge base for author names, generating the distantly supervised training set $\mathcal{T}$ requires the labeling of author names in the references.
A number of challenges arise from this task.
\itodo{variations of author names}
First, author names can appear in a reference in a variety of ways\todo{example 4?}.
Another aspect that requires attention is the possible overlapping of matches \todo{example 4?}.
Instead of deciding for one of the overlapping author names, our goal is to consider all author names.
In \Cref{subsec:i-author-name-matching} we will discuss that tree based markup languages such as \gls{xml} are less suitable to mark overlapping entries.
Instead, we will use a graph structure called \gls{goddag} which supports overlapping entries.
\todo{example}

\section{Building \glsentryshort{ge} Constraints}\label{sec:ae-training-crfs}

After labeling the occurrences of author in our reference sections, we now want to derive \gls{ge} constraints that will be used for learning a \gls{crf} model.

A first step is to specify the possible labelings $Val(Y_n)$ for a \gls{target variable} $Y_n$.
One goal in the scenario is to recognize the first names and last names of authors as such.
For this we use the labels $\texttt{FN}$ and $\texttt{LN}$, respectively.
Every other word is marked with the label $\texttt{O}$ for other.
Since our second goal is to group first names and last names together to form author names, it is important to additionally encode the beginning and end of an author name in the given word sequence.
A common approach is to extend the label by this information~\citep{houngbo2012method}.
Given the labels \texttt{FN}, \texttt{LN}, and \texttt{O} we add the following prefixes:
\begin{itemize}
  \item \texttt{B-} marks the beginning word of an author name
  \item \texttt{I-} marks an intermediate word in an author name
  \item \texttt{E-} marks the ending word in an author name (optional)
\end{itemize}
This results in labels such as \texttt{B-LN} which marks a word as last name and the beginning of an author name.
\itodo{examples}
We refer to this labeling format as the \acrfull{bieo} format since a label either has one of the three mentioned prefixes or is the \texttt{O} label.
Instead of marking the ending word of an author name using the \texttt{E-} prefix, it can also be marked as such using the \texttt{I-} prefix.
This is possible because the word following this ending word is either the beginning of the next author name (labeled with \texttt{B-FN} or \texttt{B-LN}) or a non-author word (labeled with \texttt{O}).
\itodo{example}
When leaving out the \texttt{E-} prefix, labels either have one of the two prefixes \texttt{B-} or \texttt{I-}, or consist of the \texttt{O} label.
Thereby, \citet{houngbo2012method} refer to this as the \acrfull{bio} format.

Having defined the \gls{bieo} and \gls{bio} format, the following question arises:
\researchquestiontwo%
In the following we will consider the \gls{bio} format for simplicity reasons.
Yet, the discussed approaches can also be applied to the \gls{bieo} format.

\bigskip

Given a training data set $\mathcal{T}$ (see \cref{sec:ae-distant-supervision}), we now want to generate \gls{ge} constraints for the \gls{crf} model learning.
Assuming the \gls{bio} format, a \gls{target variable} $Y_n$ has the following possible assignments:
\begin{equation*}
  Val(Y_n)=\{\texttt{B-FN},\texttt{B-LN},\texttt{I-FN},\texttt{I-LN},\texttt{O}\}
\end{equation*}
We thereby want to build a \gls{probability distribution} $P$ for $Y_n$ which assigns a probability to each label in $Val(Y_n)$ based on $\mathcal{T}$.

For this we iterate over the sequence of words in $P$.
For $w_n$ we distinguish two cases:
\begin{enumerate}
  \item $w_n$ is tagged as part of at least one author
  \item $w_n$ is not tagged as part of at least one author
\end{enumerate}

In the first case, the tagging contributes a probability mass of one to the according labeling of this word.
\itodo{example}

For the second case we consider two approaches.
The first approach is to add a probability mass of one to the \texttt{O} label for the given word.
The second approach is to add a probability mass to the different labels which sum to one.
Such a distribution can either be predefined for every label in $Val(Y_n)$ or it can be estimated from the distribution of author labels.
For the estimation we additionally need as an input the proportion of author tags to other tags.

\itodo{example}

The two different approaches for assigning a probability mass to a $w_n$ that was not matched to an author result in the following research question:
\researchquestionthree%

In our scenario we have that the number of $w_n$ which have at least one matching author name is considerably smaller than the number of $w_n$ which do not have a matching author.
It is thereby reasonable to consider dropping some of the latter $w_n$ for the construction of \gls{ge} constraints.
This gives us another research question:
\researchquestionfour%

\bigskip

After generating counts based on the two cases, they are normalized by the total count for each word.
\itodo{example}

\section{Learning \glsentryshortpl{crf}}\label{sec:ae-learning-crfs}

After generating a number of \gls{ge} constraints we now focus on the \gls{crf} model learning.
One important aspect is the graphical structure $\mathcal{\tilde{K}}$ which, as stated in \Cref{sec:learning-crfs}, is given as an input to the model learning.
Another important aspect for the model performance is the selection of suitable textual features.

\subsection{Graph Construction}\label{subsec:ae-graph-construction}

The underlying graph $\mathcal{\tilde{K}}$ of a \gls{crf} model $\mathcal{\tilde{M}}$ has a strong impact on both the runtime performance and the quality of the model.
As we discussed in \Cref{sec:learning-crfs}, an efficient inferencing is crucial for the learning of \gls{crf}.
The forward-backward algorithm allows an efficient inferencing but required $\mathcal{\tilde{K}}$ to follow a certain sequential structure.
Given our scenario, a sequential structure where neighboring words in a reference string are also connected in $\mathcal{\tilde{K}}$ is a popular choice.\todo{other papers, peng\dots}


\itodo{on linear chain crfs, markov order 0 and 1, research question}
\itodo{on $\mathbf{Y}$}

\subsection{Feature Engineering}\label{subsec:ae-fearture-engineering}

Based on the graphical structure of a \gls{linear-chain crf} with its given set of \glspl{target variable} $\mathbf{Y}$, we now consider the construction of the set of \glspl{observed variable} $\mathbf{X}$.
These \glspl{observed variable} provide information about certain textual features of the given word sequence.
As discussed in \Cref{sec:definition-crfs}, a key strength of \glspl{crf} is that \glspl{observed variable} in $\mathbf{X}$ can be highly dependent on each other without impacting the model performance.

\itodo{On feature engineering (\citet{purver2012experimenting}), also: conjunctions}

Since previous research focused on the extraction of reference string information using \glspl{crf}, there are a number of features that were suggested for this task.
\citet{peng2004accurate} distinguish between local features, layout features, and external lexicon features.



