\chapter{Author Extraction}\label{cha:author-extraction}

In this chapter we will discuss author extraction as a concrete use case for the usage of \glspl{crf} in combination with \gls{distant supervision}.
This can be broken down into to following steps:
\begin{enumerate}
  \item Preprocessing of research papers to extract reference sections as text from \gls{pdf} documents
  \item Generating tagged training sets for distant supervision
  \item Building \acrfull{ge} constraints
  \item Learning a \gls{crf} model using \gls{ge} constraints
\end{enumerate}
In the following sections we will describe these steps in more detail.

\section{Preprocessing}\label{sec:ae-preprocessing}

Before building a training set using \gls{distant supervision}, an unlabeled training data set needs to be generated.
Since the research papers that we use as the source for our unlabeled data is given in the \gls{pdf} format, a first step is to extract the textual content of the documents.

Since we want to learn a model that is able to extract author names in reference sections, it is crucial to remove the text that is not part of the reference section.
Otherwise, the learning (see \Cref{subsec:author-name-matching}) would also consider names that appear in the body of the research paper.

\section{Generating Training Sets with Distant Supervision}\label{sec:ae-distant-supervision}

As discussed in \todo{ref}, distant supervision is \dots
In this section we discuss our approach of building such a distantly supervised training set in our use case of author name extraction.
\itodo{extend}

\subsection{Knowledge Base}\label{subsec:ae-knowledge-base}

In order to apply distant supervision to the task of labeling author names, an external source for author names is needed.
Since the goal is to distinguish between the first names and last names of an author, external sources that provide this distinction are preferable.
This is due to the fact that determining which part of a name belongs to the first names and which to the last names is not trivial\todo{example}.
In addition to labeling reference sections for the distant supervision, such a knowledge base can also be used to construct features for the \gls{crf} model.
More precisely, we can derive two feature functions, $f_{\text{firstName}}(x_n)$ and $f_{\text{lastName}}(x_n)$, which return for a given word a likelihood of this word being a first name or a last name respectively.\todo{example}

\bigskip

One question that arises is whether or not the origin of the author list is of importance.
More specifically, does labeling the reference sections using an author list from a similar research domain improve the performance of the final \gls{crf} model?\todo{ref to eval}

\subsection{Author Name Matching}\label{subsec:ae-author-name-matching}

Given a data set of unlabeled reference sections and a knowledge base for author names, generating the distantly supervised training set requires the labeling of author names in the references.
A number of challenges arise from this task.
\itodo{variations of author names}
First, author names can appear in a reference in a variety of ways\todo{example}.
Another aspect that requires attention is the possible overlapping of matches \todo{example}.

\section{Building \glsentryshort{ge} Constraints}\label{sec:ae-training-crfs}

After labeling the occurrences of author in our reference sections we now want to derive \gls{ge} constraints that will be used for learning a \gls{crf} model.

For this we first need to specify the possible labelings $Val(Y_n)$ for a \gls{target variable} $Y_n$.
In our scenario we are interested in the first names and last names of authors.
For this we use the labels $\texttt{FN}$ and $\texttt{LN}$, respectively.
Every other token\todo{token?} is marked with the label $\texttt{O}$ for other.
Since our goal is to identify first names and last names of individual authors, it is important to additionally encode the beginning and the end of an author name in the given sequence.
A common approach is to extend the label by this information~\citep{houngbo2012method}.
Given the labels \texttt{FN}, \texttt{LN}, and \texttt{O} we add the following prefixes:
\begin{itemize}
  \item \texttt{B-} for the beginning word of an author name
  \item \texttt{I-} for an intermediate word in an author name
  \item \texttt{E-} for the ending word in an author name (optional)
\end{itemize}
This results in labels such as \texttt{B-LN} when an author name begins with a last name or \texttt{E-FN} when it ends with a first name.
Instead of marking the end of an author name using the \texttt{E-} prefix, it can also be marked as such using the \texttt{I-} prefix.
This is possible because the next word following the author name is either the beginning of the next author name (labeled \texttt{B-FN} or \texttt{B-LN}) or a non-author word labeled with \texttt{O}.
Since the resulting labels either have one of the prefixes \texttt{B-} or \texttt{I-}, or consist of the \texttt{O} label, \citet{houngbo2012method} refer to this as the \gls{bio} format.
\itodo{example}
\itodo{discuss given ratio of other to author labels and deriving fn/ln ratios vs.\ assuming unlabeled data to be 100\% other + research question}
\itodo{discuss ratio of labeled tokens vs.\ unlabeled tokens in constraints + research question}


\section{Learning \glsentryshortpl{crf}}\label{sec:ae-learning-crfs}

\itodo{On feature engineering (\citet{purver2012experimenting}), also: conjunctions}
\itodo{On Markov orders}
