\chapter{Distant Supervision}\label{cha:distant-supervision}

A common approach to the learning of \glspl{crf} is to use manually labeled instances.
Manual labeling results in an accurate labeling in most cases but is expensive and can thereby only be performed on small data sets.

%TODO general words on distant supervision: jiang2012information

The approach of \gls{distant supervision} allows the automated labeling of large data sets by using heuristics and external sources of information.

In this chapter we will first give an overview of \gls{distant supervision} by discussing the past usages.
We will then discuss approaches on how \glspl{crf} can be learned using distantly supervised data sets.

\section{Overview}

%TODO explain origin
Before explaining our usage of the term \gls{distant supervision}, we will first examine its origin and how it was previously used.

\bigskip

As discussed in \Cref{cha:related-work}, the term \gls{distant supervision} was introduced by \citet{mintz2009distant} as an approach for relation extraction without labeled data.
They state that \gls{distant supervision} extends the paradigm used by \citet{snow2005learning} for the extraction of hypernyms.
This is done by learning dependency paths from hypernym/hyponym word pairs that were extracted from WordNet~\citep{snow2005learning}.
The dependency paths are then used as features in a logistic regression classifier with the task of identifying hypernym pairs in a corpus~\citep{snow2005learning}.
Additionally, \citet{mintz2009distant} mention a similarity of \gls{distant supervision} to the usage of weakly labeled data in bioinformatics.
One mentioned example is how \citet{craven1999constructing} extract relations between biological objects such as proteins, cell-types, and diseases from a text corpus.
For this they train a Naive Bayes classifier with data from the Yeast Protein Database~\citep{payne1997yeast}.
\citet{surdeanu2012multi} go as far as saying that distant supervision for \gls{ie} was introduced by \citet{craven1999constructing}.

While not giving a definition of the term, \citet{mintz2009distant} state that ``[t]he intuition of distant supervision is that any sentence that contains a pair of entities that participate in a known Freebase relation is likely to express that relation in some way.''

There is now a body of research that uses the paradigm of \citet{mintz2009distant} for relation extraction~\citep{benson2011event,ritter2011named,nguyen2011end,takamatsu2012reducing,xu2013filling}.

\bigskip

Another intuition of \gls{distant supervision} that is not based on leveraging existing knowledge bases such as FreeBase.
\citet{go2009twitter} use emoticons in Twitter\footnote{\url{https://twitter.com/} (accessed May~19,~2016)} tweets as noisy labels to build the training set for sentiment analysis~\citep{go2009twitter}.

There is again a body of research that extends this approach to sentiment analysis~\citep{purver2012experimenting,marchetti2012learning,suttles2013distant}.

\citet{fan2015detecting} also apply distant supervision without using a knowledge base.
They rely on a simple heuristic for localizing tables by considering the context around a line starting with ``Table'' or ``Tab.'' as a potential table.

%TODO ling2012fine on entity recognition using distant supervision
\bigskip

Due to the different applications of \gls{distant supervision}, a precise definition\dots

What all approaches have in common is that they use some heuristic to assign labels to previously unlabeled data. These labelings are are used during the training, either in addition to preexisting labeled data or on their own.



\section{Distant Supervision and \glsentryshortpl{crf}}

Data sets that are used within a distantly supervised learning approach are typically incompletely annotated.
This is due to the fact that, in practice, external knowledge bases can not cover all observed cases in the training set.
As a result, conventional \gls{crf} learning algorithms cannot be directly applied to such data sets since they require a fully annotated input~\citep{tsuboi2008training}.

To make this more clear, recall that for \glspl{crf} we have $\bm{D}_k\not\subseteq\bm{X}$ for every $k=1,\dots,K$ (see \Cref{sec:definition-crfs}).
In other words, every set of \glspl{random variable} $\bm{D}_k$ needs to contain at least one $Y_n\in\bm{Y}$.
Otherwise, the term containing $\bm{D}_k$ cancels out during the calculation of $P(\bm{Y}\mid\bm{X})$ due to the normalization constant $Z(\bm{X})$ (see \todo{add ref} for an example).
Thereby the question arises on how data sets are handled where not every element is labeled.

In this section we will discuss approaches that use incompletely annotated data for the learning of \glspl{crf}.
\itodo{sentence on tsuboi approach?}
A very promising approach, especially for our author extraction use case (see \Cref{cha:author-extraction}), is \gls{ge}, proposed by \citet{mann2007simple}.

\subsection{Marginalization}
\citet{tsuboi2008training} propose a method for training \glspl{crf} using incomplete annotations.
They consider two cases of incomplete annotations: Partial and ambiguous annotations.
The idea is to firstly generate a set of all possible sequences with respect to the missing or ambiguous data.
\dots\todo{continue}

Yet, the resulting equations is not a concave function which prevents efficient global maximization~\citep{tsuboi2008training}.
Another challenge that arises is the amount of label sequences that are generated this way which is exponential in the number of unknown labels~\citep{tsuboi2008training}.
This problem is addressed by applying the Markov assumption and using a modification of the Forward-Backward algorithm~\citep{tsuboi2008training}.
\citet{tsuboi2008training} apply their approach on Japanese word segmentation using partial annotations and on \gls{pos} tagging using ambiguous annotations.
For the word segmentation task the proposed approach was compared with two other methods that can handle partial annotations.
The first one is filling unknown labels by prediction based on the partial annotations and the second one is training a point-wise classifier that excludes label correlations~\citep{tsuboi2008training}.
The reported results show that the proposed method outperforms both methods for various sample sizes.
For the \gls{pos} tagging they used the Penn treebank corpus \citep{marcus1993building} which contains a number of ambiguous \gls{pos} tags.
The proposed approach was compared with three other heuristics, namely random selection, selecting the first tag in the description order, and selecting the most frequent tag in the corpus.
In this case, the proposed method only moderately improved the performance which the authors believe it due to the relatively low percentage of ambiguous tags in the corpus~\citep{tsuboi2008training}.

\subsection{Generalized Expectation (\glsentryshort{ge})}

Instead of ``filling the gaps'' of incomplete annotations, \citet{mann2008generalized} apply the concept of \acrfull{ge} on the training of linear-chain \glspl{crf}.
\Gls{ge} was first proposed in \citet{mann2007simple} under the name \gls{expectation regularization} as a method for semi-supervised learning.
%TODO definition GE

\citet{mann2008generalized} use \gls{ge} criteria to train \glspl{crf} on labeled features instead of fully labeled instances.

%TODO define \mathcal{U}
In general, a \gls{ge} criterion $G(\bm{\tilde{\theta}}:\mathcal{U})$ is a score function $S$ which is defined as~\citep{mann2010generalized}:
\begin{equation}
  \label{equ:generalized-expectation}
  G(\bm{\tilde{\theta}}:\mathcal{U})=S\left(E_{\mathcal{U}}\left[E_{P(\bm{Y}\mid\bm{X})}\left[G(\bm{X},\bm{Y})\right]\right]\right)
\end{equation}
Here, $P(\bm{Y}\mid\bm{X})$ is a \gls{cpd} based on the model $\tilde{\mathcal{M}}=\{\tilde{\mathcal{K}},\bm{\tilde{\theta}}\}$.
$\mathcal{U}=\left\{\bm{u}^{(1)},\dots,\bm{u}^{(M)}\right\}$ is a data set of $M$ unlabeled instances such that $\bm{u}^{(m)}=\bm{X}^{(m)}$ (compare with $\mathcal{D}$ in \Cref{sec:learning-crfs}).
$G(\bm{X},\bm{Y})$ is given as a constraint function.

\itodo{general text on GE equation}

To clarify this general definition we now look at one possible scoring function $S$, namely the $\text{KL}$-divergence\todo{intro to KL}.
%TODO intro text on KL
Applying the $\text{KL}$-divergence as a scoring function to \Cref{equ:generalized-expectation} we have~\citep{mann2010generalized}
\begin{equation}
  \label{equ:generalized-expectation-kl}
  G(\bm{\tilde{\theta}}:\mathcal{U})=D\left(\tilde{g}_{\left(\bm{X},\bm{Y}\right)}\mid\mid E_{\mathcal{U}}\left[P(\bm{Y}\mid\bm{X})G(\bm{X},\bm{Y})\right]\right)
\end{equation}
where $\tilde{g}_{\left(\bm{X},\bm{Y}\right)}$ expresses an expectation for $\{\bm{X},\bm{Y}\}$, either in the form of a particular value or over a \gls{marginal distribution} $\tilde{P}\left(\bm{Y}\right)$~\citep{mann2010generalized}.
The result of $D$ describes the divergence between the expectation of a given constraint $\tilde{g}_{\left(\bm{X},\bm{Y}\right)}$ and the expectation over the cases of $\left\{\bm{X},\bm{Y}\right\}$ $\mathcal{U}$ with respect to the modeled \gls{cpd} $P(\bm{Y}\mid\bm{X})$\todo{rewrite}.
%target distribution: distribution for every constraint?
%expectations: calculated for every constraint separately (mallet code)?
\itodo{text on GE not being used with log-likelihood (like in mann2010) but alone}
Further, \citet{mann2010generalized} use the term \gls{label regularization} for the case when the constraint function
\begin{equation}
  \label{equ:label-regularization-constraint-function}
  G(\bm{X},\bm{Y})=\bm{\mathbbm{1}}(\bm{Y})
\end{equation}
and the constraint
\begin{equation}
  \label{equ:label-regularization-constraints}
  \tilde{g}_{\left(\bm{X},\bm{Y}\right)}=E[\tilde{P}(\bm{Y})]
\end{equation}
are used in the \gls{ge} term in \Cref{equ:generalized-expectation-kl}.
Here, $\tilde{P}(\bm{Y})$ is an estimated \gls{marginal distribution} over a set of labels $\bm{Y}$ that is given as an input to the learner.

\bigskip

There are a number of ways in which \gls{ge} constraints can be applied to an \gls{objective function}.
\citet{mann2010generalized} discuss, in the context of semi-supervised learning, the addition of \gls{ge} criteria $G(\bm{\tilde{\theta}}:\mathcal{U})$ to a likelihood function $\mathcal{L}(\bm{\tilde{\theta}}:\mathcal{D})$:
\begin{equation}
  \label{equ:objective-function-l-g}
  O(\bm{\tilde{\theta}}:\mathcal{D},\mathcal{U})=\mathcal{L}(\bm{\tilde{\theta}}:\mathcal{D})+G(\bm{\tilde{\theta}}:\mathcal{U})
\end{equation}
This way, both a labeled data set $\mathcal{D}$ and an unlabeled data set $\mathcal{U}$ can be utilized during the learning of $\bm{\tilde{\theta}}$.

It is also possible to build an \gls{objective function} only using an unlabeled data set $\mathcal{U}$.
For this we use the \gls{ge} criteria\todo{plural?}, combined with a Gaussian prior (see \Cref{equ:gaussian-prior}) to prevent overfitting:
\begin{equation}
  \label{equ:objective-function-g}
  O(\bm{\tilde{\theta}}:\mathcal{U})=G(\bm{\tilde{\theta}}:\mathcal{U})+Gauss(\bm{\theta})
\end{equation}
Such an approach was first discussed in \citet{mann2008generalized}.

Seen from a different perspective, using \gls{ge} as an objective function allows us to express expectations on a subset of elements from an unlabeled data set while other elements remain unconstrained~\citep{mann2010generalized}.
This is precisely what is needed in order to apply \gls{distant supervision} to the learning of \glspl{crf}.
The idea is to generate \glspl{marginal distribution} $\tilde{P}(\bm{Y})$ (see \Cref{equ:label-regularization-constraints}) for the $\bm{Y}$ for which we have information from an external source.

In the following chapter we will elaborate on this idea with the goal of extraction of authors from the reference section of research papers.

