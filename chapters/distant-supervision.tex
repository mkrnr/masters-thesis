\chapter{Distant Supervision}\label{cha:distant-supervision}

A common approach to the learning of \glspl{crf} is to use manually labeled instances.
Manual labeling results in an accurate labeling in most cases but is expensive and can thereby only be performed on small data sets.

%TODO general words on distant supervision: jiang2012information

The approach of \gls{distant supervision} allows the automated labeling of large data sets by using heuristics and external sources of information.

In this chapter we will first give an overview of \gls{distant supervision} by discussing the past usages.
We will then discuss approaches on how \glspl{crf} can be learned using distantly supervised data sets.

\section{Overview}

Before explaining our usage of the term \gls{distant supervision}, we will first examine its origin and how it was previously used.

\bigskip

As discussed in \Cref{cha:related-work}, the term \gls{distant supervision} was introduced by \citet{mintz2009distant} as an approach for relation extraction without labeled data.
They state that \gls{distant supervision} extends the paradigm used by \citet{snow2005learning} for the extraction of hypernyms.
This is done by learning dependency paths from hypernym/hyponym word pairs that were extracted from WordNet~\citep{snow2005learning}.
The dependency paths are then used as features in a logistic regression classifier with the task of identifying hypernym pairs in a corpus~\citep{snow2005learning}.
Additionally, \citet{mintz2009distant} mention a similarity of \gls{distant supervision} to the usage of weakly labeled data in bioinformatics.
One mentioned example is how \citet{craven1999constructing} extract relations between biological objects such as proteins, cell-types, and diseases from a text corpus.
For this they train a Naive Bayes classifier with data from the Yeast Protein Database~\citep{payne1997yeast}.
\citet{surdeanu2012multi} go as far as saying that distant supervision for \gls{ie} was introduced by \citet{craven1999constructing}.

While not giving a definition of the term, \citet{mintz2009distant} state that ``[t]he intuition of distant supervision is that any sentence that contains a pair of entities that participate in a known Freebase relation is likely to express that relation in some way.''

There is now a body of research that uses the paradigm of \citet{mintz2009distant} for relation extraction~\citep{benson2011event,ritter2011named,nguyen2011end,takamatsu2012reducing,xu2013filling}.

\bigskip

Another intuition of \gls{distant supervision} that is not based on leveraging existing knowledge bases such as FreeBase.
\citet{go2009twitter} use emoticons in Twitter\footnote{\url{https://twitter.com/} (accessed May~19,~2016)} tweets as noisy labels to build the training set for sentiment analysis~\citep{go2009twitter}.

There is again a body of research that extends this approach to sentiment analysis~\citep{purver2012experimenting,marchetti2012learning,suttles2013distant}.

\citet{fan2015detecting} also apply distant supervision without using a knowledge base.
They rely on a simple heuristic for localizing tables by considering the context around a line starting with ``Table'' or ``Tab.'' as a potential table.

%TODO ling2012fine on entity recognition using distant supervision
\bigskip

Due to the different applications of \gls{distant supervision}, a precise definition\dots

What all approaches have in common is that they use some heuristic to assign labels to previously unlabeled data. These labelings are are used during the training, either in addition to preexisting labeled data or on their own.

\section{Distant Supervision and \glsentryshortpl{crf}}

Data sets that are used within a distantly supervised learning approach are typically incompletely annotated.
This is due to the fact that, in practice, external knowledge bases can not cover all observed cases in the training set.
As a result, conventional \gls{crf} learning algorithms cannot be directly applied to such data sets since they require a fully annotated input~\citep{tsuboi2008training}.

To make this more clear, recall that for \glspl{crf} we have $\bm{D}_k\not\subseteq\bm{X}$ for every $k=1,\dots,K$ (see \Cref{sec:definition-crfs}).
In other words, every set of \glspl{random variable} $\bm{D}_k$ needs to contain at least one $Y_n\in\bm{Y}$.
Otherwise, the term containing $\bm{D}_k$ cancels out during the calculation of $P(\bm{Y}\mid\bm{X})$ due to the normalization constant $Z(\bm{X})$ (see \todo{add ref} for an example).
Thereby the question arises on how data sets are handled where not every element is labeled.

In this section we will discuss two approaches that use incompletely annotated data for the learning of \glspl{crf}.
First we look at the approach by \citet{tsuboi2008training} who a marginalization over all possible sequences given an incompletely annotated sequence.
Then we will present the very promising approach of \acrfull{ge} proposed by \citet{mann2007simple} which provides an alternative \gls{objective function} for unlabeled data based on \gls{label regularization}.

\subsection{Marginalization}

\citet{tsuboi2008training} propose a method for training \glspl{crf} using incomplete annotations based on marginalization.
Given a sequence of length $M$, an incomplete annotation $\bm{L}$ is defined as a sequence of subsets of possible assignments to the \gls{target variable} at each position $m$~\citep{tsuboi2008training}:
\begin{equation}
  \label{equ:incomplete-annotation}
  \begin{split}
  \bm{L}=& \left\{L_1,\dots,L_M\right\}\\
  L_m=& \left\{\tilde{y}_1,\dots,\tilde{y}_T\right\}\ \ \ \ \ \ \ \ \tilde{y}_t\in Val(Y_t)
  \end{split}
\end{equation}
\itodo{example}
Using the definition of $\bm{L}$ we can now specify two cases of incomplete annotations: Partial annotations and ambiguous annotations.
If an element at position $m$ in a sequence is not annotated then the corresponding $L_m$ contains all possible labelings: $L_m=Val(Y_m)$\todo{fix notation?}.
Ambiguous annotations are represented such that $L_m$ contains the set of ambiguous annotations for the corresponding element.
\itodo{example}

\bigskip

We can now formalize the marginalization approach which allows the application of an incomplete annotation $\bm{L}$ to a \gls{crf} model~\citep{tsuboi2008training}:
\begin{equation}
  \label{equ:crf-marginalization}
  \begin{split}
    P\left(\bm{Y}_{\bm{L}}\mid\bm{X}\right)=\sum_{\bm{y}\in\bm{Y}_{\bm{L}}}P\left(\bm{Y}=\bm{y}\mid\bm{X}\right)
  \end{split}
\end{equation}
Here, $\bm{Y}_{\bm{L}}$ is defined as the set of all possible assignments $\bm{y}$ to $\bm{Y}$ which are consistent with $\bm{L}$~\citep{tsuboi2008training}.
Using this approach, we can still apply all concepts for encoding, inferencing, and learning \glspl{crf} that we discussed in \Cref{cha:crfs}.

\bigskip

Yet, there can be a number of possible issues with this marginalization approach.
One such issue is that, in case of partial annotations, the number of generated assignments $\bm{y}$ in $\bm{Y}_{\bm{L}}$ is exponential in the number of unannotated elements $U$ in the sequence~\citep{tsuboi2008training}.
Assuming that every unannotated element $m$ has the same number of possible assignments we have
\begin{equation}
  \label{equ:marginalization-number-of-assingments}
  |\bm{Y}_{\bm{L}}|=T^U
\end{equation}
where $T=|Val\left(Y_m\right)|$ for some $1\leq m\leq M$.
The worst case is $|\bm{Y}_{\bm{L}}|=T^M$ when no element in the sequence is annotated.
\citep{tsuboi2008training} argue that this problem can be addressed by applying the Markov assumption and using a modification of the forward-backward algorithm (see \Cref{sec:inference-crfs}).


Another issue is that the log-likelihood function that is derived from \Cref{equ:crf-marginalization} is not concave~\citep{tsuboi2008training}.
Thereby, during the maximum log-likelihood estimation, a local maximum is not necessarily a global maximum which needs to be considered during the learning phase.

Further, when considering the marginalization of ambiguous annotations, it is not possible to provide additional information on the distribution of possible assignments.
In this form, the model does not allow us to specify a \gls{marginal distribution} over the elements in $L_m$ which is considered during the marginalization.
This is especially problematic when considering the implementation of a distantly supervised approach.
Distantly supervised labelings will often be ambiguous but it is possible to derive the probabilities for the different possible labels from the external source of information.
In \Cref{cha:author-extraction} we will discuss a concrete example for such a scenario.

\subsection{Generalized Expectation (\glsentryshort{ge})}

Instead of marginalizing incomplete annotations, \citet{mann2008generalized} apply the concept of \acrfull{ge} on the training of linear-chain \glspl{crf}.
\Gls{ge} was first proposed in \citet{mann2007simple} under the name \gls{expectation regularization} as a method for semi-supervised learning.

%TODO definition GE

%TODO define \mathcal{U}
In general, a \gls{ge} criterion $G(\bm{\tilde{\theta}}:\mathcal{U})$ is a score function $S$ which is defined as~\citep{mann2010generalized}:
\begin{equation}
  \label{equ:generalized-expectation}
  G(\bm{\tilde{\theta}}:\mathcal{U})=S\left(E_{\mathcal{U}}\left[E_{P(\bm{Y}\mid\bm{X})}\left[G(\bm{X},\bm{Y})\right]\right]\right)
\end{equation}
Here, $P(\bm{Y}\mid\bm{X})$ is a \gls{cpd} based on the model $\tilde{\mathcal{M}}=\{\tilde{\mathcal{K}},\bm{\tilde{\theta}}\}$.
$\mathcal{U}=\left\{\bm{u}^{(1)},\dots,\bm{u}^{(M)}\right\}$ is a data set of $M$ unlabeled instances such that $\bm{u}^{(m)}=\bm{X}^{(m)}$ (compare with $\mathcal{D}$ in \Cref{sec:learning-crfs}).
$G(\bm{X},\bm{Y})$ is given as a constraint function.

\itodo{general text on GE equation}

To clarify this general definition we now look at one possible scoring function $S$, namely the \acrshort{kl} divergence.
The \acrfull{kl} divergence,\dots, is defined as~\citep{mackay2003information}
\begin{equation}
  \label{equ:kl-divergence}
  D_{\text{KL}}(P\mid\mid Q)=\sum_x \frac{P(x)}{Q(x)}
\end{equation}
where the \glspl{probability distribution} $P$ and $Q$ are defined over the same alphabet\todo{introduce}.

Applying the $\text{KL}$-divergence as a scoring function $D_{\text{KL}}$ to \Cref{equ:generalized-expectation} we have~\citep{mann2010generalized}
\begin{equation}
  \label{equ:generalized-expectation-kl}
  G(\bm{\tilde{\theta}}:\mathcal{U})=D_{\text{KL}}\left(\tilde{g}_{\left(\bm{X},\bm{Y}\right)}\mid\mid E_{\mathcal{U}}\left[P(\bm{Y}\mid\bm{X})G(\bm{X},\bm{Y})\right]\right)
\end{equation}
where $\tilde{g}_{\left(\bm{X},\bm{Y}\right)}$ expresses an expectation for $\{\bm{X},\bm{Y}\}$, either in the form of a particular value or over a \gls{marginal distribution} $\tilde{P}\left(\bm{Y}\right)$~\citep{mann2010generalized}.
The result of $D_{\text{KL}}$ describes the divergence between the expectation of a given constraint $\tilde{g}_{\left(\bm{X},\bm{Y}\right)}$ and the expectation over the cases of $\left\{\bm{X},\bm{Y}\right\}$ $\mathcal{U}$ with respect to the modeled \gls{cpd} $P(\bm{Y}\mid\bm{X})$\todo{rewrite}.
%target distribution: distribution for every constraint?
%expectations: calculated for every constraint separately (mallet code)?
\itodo{text on GE not being used with log-likelihood (like in mann2010) but alone}
Further, \citet{mann2010generalized} use the term \gls{label regularization} for the case when the constraint function
\begin{equation}
  \label{equ:label-regularization-constraint-function}
  G(\bm{X},\bm{Y})=\bm{\mathbbm{1}}(\bm{Y})
\end{equation}
and the constraint
\begin{equation}
  \label{equ:label-regularization-constraints}
  \tilde{g}_{\left(\bm{X},\bm{Y}\right)}=E[\tilde{P}(\bm{Y})]
\end{equation}
are used in the \gls{ge} term in \Cref{equ:generalized-expectation-kl}.
Here, $\tilde{P}(\bm{Y})$ is an estimated \gls{marginal distribution} over a set of labels $\bm{Y}$ that is given as an input to the learner.

\bigskip

There are a number of ways in which \gls{ge} constraints can be applied to an \gls{objective function}.
\citet{mann2010generalized} discuss, in the context of semi-supervised learning, the addition of \gls{ge} criteria $G(\bm{\tilde{\theta}}:\mathcal{U})$ to a likelihood function $\mathcal{L}(\bm{\tilde{\theta}}:\mathcal{D})$:
\begin{equation}
  \label{equ:objective-function-l-g}
  O(\bm{\tilde{\theta}}:\mathcal{D},\mathcal{U})=\mathcal{L}(\bm{\tilde{\theta}}:\mathcal{D})+G(\bm{\tilde{\theta}}:\mathcal{U})
\end{equation}
This way, both a labeled data set $\mathcal{D}$ and an unlabeled data set $\mathcal{U}$ can be utilized during the learning of $\bm{\tilde{\theta}}$.

It is also possible to build an \gls{objective function} only using an unlabeled data set $\mathcal{U}$.
For this we use the \gls{ge} criteria\todo{plural?}, combined with a Gaussian prior (see \Cref{equ:gaussian-prior}) to prevent overfitting:
\begin{equation}
  \label{equ:objective-function-g}
  O(\bm{\tilde{\theta}}:\mathcal{U})=G(\bm{\tilde{\theta}}:\mathcal{U})+Gauss(\bm{\theta})
\end{equation}
Such an approach was first discussed in \citet{mann2008generalized}.

Seen from a different perspective, using \gls{ge} as an objective function allows us to express expectations on a subset of elements from an unlabeled data set while other elements remain unconstrained~\citep{mann2010generalized}.
This is precisely what is needed in order to apply \gls{distant supervision} to the learning of \glspl{crf}.
The idea is to generate \glspl{marginal distribution} $\tilde{P}(\bm{Y})$ (see \Cref{equ:label-regularization-constraints}) for the $\bm{Y}$ for which we have information from an external source.

In the following chapter we will elaborate on this idea with the goal of extraction of authors from the reference section of research papers.

