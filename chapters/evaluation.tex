\chapter{Evaluation}\label{cha:evaluation}

In this chapter, we will discuss our evaluation with the goal of answering the research questions in \Cref{cha:author-extraction}.

Three typical metrics for assessing the performance of a sequence tagging task are \gls{precision}, \gls{recall}, and the \gls{f1 score}~\citep{councill2008parscit}.
Given a sequence of words where each word is assigned a label $L$ out of a set of labels, \gls{precision} and \gls{recall} are defined as~\citep{goutte2005probabilistic}:
\begin{equation*}
  \textit{precision}(L)=\frac{TP(L)}{TP(L)+FP(L)}\hspace{4em}\textit{recall}(L)=\frac{TP(L)}{TP(L)+FN(L)}
\end{equation*}
Here, $TP(L)$, $FP(L)$, and $FN(L)$ stand for the True Positive, False Positive, and False Negative taggings of label $L$, respectively.
Positive refers to the number words that are labeled with $L$ in the given tagged sequence and True refers to the number of words that are labeled with $L$ in a correctly tagged sequence.
Negative and False are defined accordingly.

To combine the two metrics into one, the \gls{f1 score} is defined as the harmonic mean of \gls{precision} and \gls{recall} \citep{bilenko2003adaptive}:
\begin{equation*}
  \textit{F1}(L)=\frac{2\cdot\textit{precision(L)}\cdot\textit{recall(L)}}{\textit{precision(L)}+\textit{recall(L)}}.
\end{equation*}

A testing set of correctly tagged reference strings was manually created.
For this, we randomly selected $250$ \glspl{pdf} out of the $32,470$ research papers from \gls{ssoar}.
We were able to extract the text from $244$ \glspl{pdf}.
From this, we selected the $54$ research papers that satisfy the following requirements:
\begin{itemize}
  \item It is written in German.
  \item It contains a reference section.
  \item its text does not show signs of errors from the \gls{pdf} extraction step.
\end{itemize}

We then manually tagged all authors in the reference section, while also distinguishing between their first names and last names.
Statistics of the resulting labels for the \gls{bio} and \gls{bieo} formats are shown in \Cref{tab:statistics-manually-tagged}.
\begin{table}[t]
\centering
\begin{minipage}[t]{0.3\linewidth}
\centering
\gls{bio} Format\par
\smallskip
\begin{tabular}{l r}
  \toprule
  Label & Count\\
  \midrule
  \texttt{B-FN}    & 551\\
  \texttt{B-LN}    & 2697\\
                   & \\
                   & \\
  \texttt{I-FN}    & 3197\\
  \texttt{I-LN}    & 609\\
  \texttt{I-O}     & 1\\
  \texttt{O}       & 33459\\
  \midrule
  Author Labels    & 7055\\
  All Labels       & 40514\\
  \bottomrule
\end{tabular}
\end{minipage}
\quad
\begin{minipage}[t]{0.3\linewidth}
\centering
\gls{bieo} Format\par
\smallskip
\begin{tabular}{l r}
  \toprule
  Label & Count\\
  \midrule
  \texttt{B-FN}    & 551\\
  \texttt{B-LN}    & 2697\\
  \texttt{E-FN}    & 2655\\
  \texttt{E-LN}    & 560\\
  \texttt{I-FN}    & 542\\
  \texttt{I-LN}    & 49\\
  \texttt{I-O}     & 1\\
  \texttt{O}       & 33459\\
  \midrule
  Author Labels    & 7055\\
  All Labels       & 40514\\
  \bottomrule
\end{tabular}
\end{minipage}
\caption{Statistics on the manually tagged data set for labels following the \gls{bio} and \gls{bieo} formats. ``Author Labels'' are ``All Labels'' minus the \texttt{O} labels.}
\label{tab:statistics-manually-tagged}
\end{table}
Note that our testing set contains exactly one word that has the label \texttt{I-O} for Intermediate Other.
This label is assigned to the misplaced comma in ``Wolff , S.'', which is part a reference string in \citet{morth1998spurensuche}.

\itodo{discussion on training set creation (removed testing files)}

\bigskip

We now discuss evaluations to address the research questions from \Cref{cha:author-extraction}.
\RQ{1} considers the impact of using a related list of author names as the knowledge base in comparison to an unrelated list.
As discussed in \Cref{subsec:i-knowledge-base-creation}, we have two different sources of author names.
The \texttt{gnd} data set contains persons in the German speaking area but has no further restrictions on the research area.
The \texttt{swp} data set, on the other hand, contains author names that are related to the research area of our unlabeled set of reference sections.
To compare the two data sources, we additionally created the \texttt{swp-trim} data set.
It contains the same total number of authors as the \texttt{gnd} data set (see \Cref{tab:knowledge-base-statistics}).
\todo{consider gnd-diff vs gnd-full?}











%TODO discuss challenges of distant supervision on our task (see reinanda2014prior)
%sutton2010, page48: gauss=10 is typical for medium-sized training sets

%on gaussian prior: Additionally, \citet{sutton2010introduction} state that small changes to $\sigma^2$, for example up to a factor of 10, do not have a big impact on the accuracy of the final model.
